#!/usr/bin/env python3
"""
ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ãƒ„ãƒ¼ãƒ«

PRFDatasetã®åˆæœŸåŒ–å‡¦ç†ã‚’å‚è€ƒã«ã—ã¦ã€ä»¥ä¸‹ã®å€¤ã‚’è¨ˆç®—ã—YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã™ã‚‹ï¼š
- src_vocab_list: ã‚½ãƒ¼ã‚¹èªå½™ãƒªã‚¹ãƒˆ
- tgt_vocab_list: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèªå½™ãƒªã‚¹ãƒˆ
- max_tgt_length: æœ€å¤§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé•·
- max_src_points: æœ€å¤§ã‚½ãƒ¼ã‚¹ç‚¹æ•°
"""

import argparse
import logging
import multiprocessing as mp
import os
import re
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Any, Dict, List, Set, Tuple

import pandas as pd
import yaml
from tqdm import tqdm


def encode_expr(expr_str: str) -> List[str]:
    """exprã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å‡¦ç†ã‚’æ¨¡æ“¬ï¼‰"""
    # é–¢æ•°åï¼ˆZ,S,P,C,Rï¼‰ã€æ•°å­—ã€æ‹¬å¼§ã€ã‚«ãƒ³ãƒã‚’æŠ½å‡ºï¼ˆç©ºç™½ã¯é™¤å¤–ï¼‰
    tokens = re.findall(r"[ZSPCRPRF]+|\d+|[\(\),]", expr_str)
    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
    tokens = [token for token in tokens if token.strip()]
    # BOS, EOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
    return ["[BOS]"] + tokens + ["[EOS]"]


def process_single_row(
    row: pd.Series, row_index: int = None
) -> Tuple[Set[str], Set[str], int, int, int, int, Dict[int, List[int]]]:
    """
    å˜ä¸€è¡Œã‚’å‡¦ç†ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹å…±é€šé–¢æ•°

    Args:
        row: pandas DataFrame ã®è¡Œ
        row_index: è¡Œã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹

    Returns:
        Tuple[src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points, max_point_dim, skipped_count, point_num_dist(Dict[int, List[int]])]
    """
    src_vocab = set()
    tgt_vocab = set()
    max_tgt_length = 0
    max_src_points = 0
    max_point_dim = 0
    skipped_count = 0
    point_num_dist = {}

    # 1. srcèªå½™ã®æŠ½å‡ºã¨point_num_distã®è¨ˆç®—ï¼ˆsourceï¼‰
    if "source" in row and pd.notna(row["source"]):
        source_str = str(row["source"])
        # æ•°å­—ã®ã¿ã‚’æŠ½å‡ºï¼ˆç©ºç™½ã€ã‚«ãƒ³ãƒã€æ‹¬å¼§ã¯é™¤ãï¼‰
        tokens = re.findall(r"\d+", source_str)
        src_vocab.update(tokens)

        # max_src_pointsã¨max_point_dimã®è¨ˆç®—ã€ãŠã‚ˆã³point_num_distã®è¨ˆç®—
        try:
            source_data = (
                eval(source_str) if isinstance(source_str, str) else source_str
            )
            points = len(source_data)
            max_src_points = max(max_src_points, points)

            # point_num_distã®è¨ˆç®—ï¼ˆsourceã®0æ¬¡å…ƒç›®ã®ã‚µã‚¤ã‚ºï¼‰
            point_count = len(source_data)
            if row_index is not None:
                if point_count not in point_num_dist:
                    point_num_dist[point_count] = []
                point_num_dist[point_count].append(row_index)

            # sourceã®å„è¦ç´ ï¼ˆ1ç•ªç›®ã®indexï¼‰ã®é•·ã•ã®æœ€å¤§å€¤ã‚’è¨ˆç®—
            if source_data and isinstance(source_data, list):
                for point in source_data:
                    if isinstance(point, list) and len(point) > 0:
                        point_dim = len(point)
                        max_point_dim = max(max_point_dim, point_dim)

        except (ValueError, SyntaxError, TypeError):
            skipped_count += 1

    # inputsã‚«ãƒ©ãƒ ã‚‚å‡¦ç†ï¼ˆãƒ‡ãƒ¼ã‚¿ã«ã‚ˆã£ã¦ã¯ã“ã¡ã‚‰ã‚’ä½¿ç”¨ï¼‰
    if "inputs" in row and pd.notna(row["inputs"]):
        inputs_str = str(row["inputs"])
        # æ•°å­—ã®ã¿ã‚’æŠ½å‡ºï¼ˆç©ºç™½ã€ã‚«ãƒ³ãƒã€æ‹¬å¼§ã¯é™¤ãï¼‰
        tokens = re.findall(r"\d+", inputs_str)
        src_vocab.update(tokens)

        # max_src_pointsã¨max_point_dimã®è¨ˆç®—ã€ãŠã‚ˆã³point_num_distã®è¨ˆç®—
        try:
            inputs_data = (
                eval(inputs_str) if isinstance(inputs_str, str) else inputs_str
            )
            points = len(inputs_data)
            max_src_points = max(max_src_points, points)

            # point_num_distã®è¨ˆç®—ï¼ˆinputsã®0æ¬¡å…ƒç›®ã®ã‚µã‚¤ã‚ºï¼‰
            point_count = len(inputs_data)
            if row_index is not None:
                if point_count not in point_num_dist:
                    point_num_dist[point_count] = []
                point_num_dist[point_count].append(row_index)

            # inputsã®å„è¦ç´ ï¼ˆ1ç•ªç›®ã®indexï¼‰ã®é•·ã•ã®æœ€å¤§å€¤ã‚’è¨ˆç®—
            if inputs_data and isinstance(inputs_data, list):
                for point in inputs_data:
                    if isinstance(point, list) and len(point) > 0:
                        point_dim = len(point)
                        max_point_dim = max(max_point_dim, point_dim)

        except (ValueError, SyntaxError, TypeError):
            skipped_count += 1

    if "outputs" in row and pd.notna(row["outputs"]):
        outputs_str = str(row["outputs"])
        # æ•°å­—ã®ã¿ã‚’æŠ½å‡ºï¼ˆç©ºç™½ã€ã‚«ãƒ³ãƒã€æ‹¬å¼§ã¯é™¤ãï¼‰
        tokens = re.findall(r"\d+", outputs_str)
        src_vocab.update(tokens)

    # 2. tgtèªå½™ã®æŠ½å‡ºã¨max_tgt_lengthã®è¨ˆç®—ï¼ˆexprï¼‰
    if "expr" in row and pd.notna(row["expr"]):
        expr_str = str(row["expr"])

        # tgtèªå½™ã®æŠ½å‡ºï¼ˆç©ºç™½ã‚’é™¤å¤–ï¼‰
        tokens = re.findall(r"[ZSPCRPRF]+|\d+|[\(\),]", expr_str)
        tgt_vocab.update(tokens)

        # max_tgt_lengthã®è¨ˆç®—
        encoded_tokens = encode_expr(expr_str)
        max_tgt_length = max(max_tgt_length, len(encoded_tokens))

    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
    src_vocab.discard("")
    tgt_vocab.discard("")

    return (
        src_vocab,
        tgt_vocab,
        max_tgt_length,
        max_src_points,
        max_point_dim,
        skipped_count,
        point_num_dist,
    )


def process_chunk(
    chunk_data: Tuple[int, pd.DataFrame],
) -> Tuple[Set[str], Set[str], int, int, int, int, Dict[int, List[int]]]:
    """
    ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰

    Args:
        chunk_data: (chunk_id, DataFrame) ã®ã‚¿ãƒ—ãƒ«

    Returns:
        Tuple[src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points, max_point_dim, skipped_count, point_num_dist]
    """
    chunk_id, chunk_df = chunk_data

    src_vocab_global = set()
    tgt_vocab_global = set()
    max_tgt_length_global = 0
    max_src_points_global = 0
    max_point_dim_global = 0
    total_skipped = 0
    point_num_dist_global = {}

    for row_index, row in chunk_df.iterrows():
        (
            src_vocab,
            tgt_vocab,
            max_tgt_length,
            max_src_points,
            max_point_dim,
            skipped_count,
            point_num_dist,
        ) = process_single_row(row, row_index)

        # çµæœã‚’ãƒãƒ¼ã‚¸
        src_vocab_global.update(src_vocab)
        tgt_vocab_global.update(tgt_vocab)
        max_tgt_length_global = max(max_tgt_length_global, max_tgt_length)
        max_src_points_global = max(max_src_points_global, max_src_points)
        max_point_dim_global = max(max_point_dim_global, max_point_dim)
        total_skipped += skipped_count

        # point_num_distã‚’ãƒãƒ¼ã‚¸
        for point_count, row_indices in point_num_dist.items():
            if point_count not in point_num_dist_global:
                point_num_dist_global[point_count] = []
            point_num_dist_global[point_count].extend(row_indices)

    return (
        src_vocab_global,
        tgt_vocab_global,
        max_tgt_length_global,
        max_src_points_global,
        max_point_dim_global,
        total_skipped,
        point_num_dist_global,
    )


def calculate_metadata_parallel(
    csv_path: str, n_workers: int = None, chunk_size: int = 1000
) -> Tuple[Set[str], Set[str], int, int, int, Dict[int, List[int]]]:
    """
    ä¸¦åˆ—å‡¦ç†ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—

    Args:
        csv_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        n_workers: ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼ˆNoneã®å ´åˆã¯CPUæ•°ï¼‰
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º

    Returns:
        Tuple[src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points, max_point_dim, point_num_dist]
    """
    if n_workers is None:
        n_workers = min(mp.cpu_count(), 8)  # æœ€å¤§8ãƒ—ãƒ­ã‚»ã‚¹

    logging.info(f"Starting parallel processing with {n_workers} workers")

    # ãƒ•ã‚¡ã‚¤ãƒ«ã®ç·è¡Œæ•°ã‚’å–å¾—
    total_rows = sum(1 for _ in open(csv_path)) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã
    logging.info(f"Total rows to process: {total_rows}")

    # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦DataFrameã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    chunks = []
    chunk_id = 0

    print("ğŸ“š Loading and splitting CSV into chunks...")
    chunk_reader = pd.read_csv(csv_path, chunksize=chunk_size)

    for chunk_df in tqdm(chunk_reader, desc="Loading chunks"):
        chunks.append((chunk_id, chunk_df))
        chunk_id += 1

    logging.info(f"Created {len(chunks)} chunks for processing")

    # ä¸¦åˆ—å‡¦ç†ã§å„ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
    src_vocab_global = set()
    tgt_vocab_global = set()
    max_tgt_length_global = 0
    max_src_points_global = 0
    max_point_dim_global = 0
    total_skipped = 0
    point_num_dist_global = {}

    print(f"âš¡ Processing {len(chunks)} chunks with {n_workers} workers...")

    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        # å…¨ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚µãƒ–ãƒŸãƒƒãƒˆ
        futures = {
            executor.submit(process_chunk, chunk): chunk[0] for chunk in chunks
        }

        # é€²æ—ãƒãƒ¼ä»˜ãã§çµæœã‚’å–å¾—
        with tqdm(
            total=len(chunks), desc="Processing chunks", unit="chunk"
        ) as pbar:
            for future in as_completed(futures):
                chunk_id = futures[future]
                try:
                    (
                        src_vocab,
                        tgt_vocab,
                        max_tgt_length,
                        max_src_points,
                        max_point_dim,
                        skipped_count,
                        point_num_dist,
                    ) = future.result()

                    # çµæœã‚’ãƒãƒ¼ã‚¸
                    src_vocab_global.update(src_vocab)
                    tgt_vocab_global.update(tgt_vocab)
                    max_tgt_length_global = max(
                        max_tgt_length_global, max_tgt_length
                    )
                    max_src_points_global = max(
                        max_src_points_global, max_src_points
                    )
                    max_point_dim_global = max(
                        max_point_dim_global, max_point_dim
                    )
                    total_skipped += skipped_count

                    # point_num_distã‚’ãƒãƒ¼ã‚¸
                    for point_count, row_indices in point_num_dist.items():
                        if point_count not in point_num_dist_global:
                            point_num_dist_global[point_count] = []
                        point_num_dist_global[point_count].extend(row_indices)

                    pbar.set_postfix(
                        src_vocab=len(src_vocab_global),
                        tgt_vocab=len(tgt_vocab_global),
                        max_tgt_len=max_tgt_length_global,
                        max_src_pts=max_src_points_global,
                        max_pt_dim=max_point_dim_global,
                        skipped=total_skipped,
                        refresh=False,
                    )

                except Exception as e:
                    logging.error(f"Error processing chunk {chunk_id}: {e}")

                pbar.update(1)

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
    if max_tgt_length_global == 0:
        max_tgt_length_global = 128
    if max_src_points_global == 0:
        max_src_points_global = 20
    if max_point_dim_global == 0:
        max_point_dim_global = 1

    if total_skipped > 0:
        logging.warning(f"Skipped {total_skipped} samples during processing")

    logging.info("Parallel processing completed:")
    logging.info(f"  - Src vocabulary size: {len(src_vocab_global)}")
    logging.info(f"  - Tgt vocabulary size: {len(tgt_vocab_global)}")
    logging.info(f"  - Max target length: {max_tgt_length_global}")
    logging.info(f"  - Max source points: {max_src_points_global}")
    logging.info(f"  - Max point dimension: {max_point_dim_global}")

    return (
        src_vocab_global,
        tgt_vocab_global,
        max_tgt_length_global,
        max_src_points_global,
        max_point_dim_global,
        point_num_dist_global,
    )


def calculate_all_metadata(
    csv_path: str,
) -> Tuple[Set[str], Set[str], int, int, int, Dict[int, List[int]]]:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’1å›ã ã‘èµ°æŸ»ã—ã¦å…¨ã¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆï¼‰

    Returns:
        Tuple[src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points, max_point_dim, point_num_dist(Dict[int, List[int]])]
    """
    src_vocab = set()
    tgt_vocab = set()
    max_tgt_length = 0
    max_src_points = 0
    max_point_dim = 0
    skipped_count = 0
    point_num_dist = {}

    try:
        chunk_size = 1000

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ç·è¡Œæ•°ã‚’å–å¾—
        total_rows = sum(1 for _ in open(csv_path)) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã
        logging.info(f"Total rows to process: {total_rows}")

        with tqdm(
            total=total_rows,
            desc="ğŸ” Processing CSV data",
            unit="rows",
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
        ) as pbar:
            for chunk in pd.read_csv(csv_path, chunksize=chunk_size):
                for row_index, row in chunk.iterrows():
                    # å…±é€šé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦è¡Œã‚’å‡¦ç†
                    (
                        row_src_vocab,
                        row_tgt_vocab,
                        row_max_tgt_length,
                        row_max_src_points,
                        row_max_point_dim,
                        row_skipped_count,
                        row_point_num_dist,
                    ) = process_single_row(row, row_index)

                    # çµæœã‚’ãƒãƒ¼ã‚¸
                    src_vocab.update(row_src_vocab)
                    tgt_vocab.update(row_tgt_vocab)
                    max_tgt_length = max(max_tgt_length, row_max_tgt_length)
                    max_src_points = max(max_src_points, row_max_src_points)
                    max_point_dim = max(max_point_dim, row_max_point_dim)
                    skipped_count += row_skipped_count

                    # point_num_distã‚’ãƒãƒ¼ã‚¸
                    for point_count, row_indices in row_point_num_dist.items():
                        if point_count not in point_num_dist:
                            point_num_dist[point_count] = []
                        point_num_dist[point_count].extend(row_indices)

                    pbar.update(1)

                    # çµ±è¨ˆæƒ…å ±ã‚’100è¡Œã”ã¨ã«æ›´æ–°
                    if pbar.n % 100 == 0:
                        pbar.set_postfix(
                            src_vocab=len(src_vocab),
                            tgt_vocab=len(tgt_vocab),
                            max_tgt_len=max_tgt_length,
                            max_src_pts=max_src_points,
                            max_pt_dim=max_point_dim,
                            skipped=skipped_count,
                            refresh=False,
                        )

    except Exception as e:
        logging.warning(f"Chunk reading failed: {e}. Trying normal reading...")
        # ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ãŒå¤±æ•—ã—ãŸå ´åˆã¯é€šå¸¸ã®èª­ã¿è¾¼ã¿
        df = pd.read_csv(csv_path)

        logging.info(f"Fallback: Processing {len(df)} rows in single batch")

        for row_index, row in tqdm(
            df.iterrows(),
            total=len(df),
            desc="ğŸ” Processing CSV data (fallback)",
            unit="rows",
        ):
            # å…±é€šé–¢æ•°ã‚’ä½¿ç”¨ã—ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯å‡¦ç†ã‚‚çµ±ä¸€
            (
                row_src_vocab,
                row_tgt_vocab,
                row_max_tgt_length,
                row_max_src_points,
                row_max_point_dim,
                row_skipped_count,
                row_point_num_dist,
            ) = process_single_row(row, row_index)

            # çµæœã‚’ãƒãƒ¼ã‚¸
            src_vocab.update(row_src_vocab)
            tgt_vocab.update(row_tgt_vocab)
            max_tgt_length = max(max_tgt_length, row_max_tgt_length)
            max_src_points = max(max_src_points, row_max_src_points)
            max_point_dim = max(max_point_dim, row_max_point_dim)
            skipped_count += row_skipped_count

            # point_num_distã‚’ãƒãƒ¼ã‚¸
            for point_count, row_indices in row_point_num_dist.items():
                if point_count not in point_num_dist:
                    point_num_dist[point_count] = []
                point_num_dist[point_count].extend(row_indices)

    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
    src_vocab.discard("")
    tgt_vocab.discard("")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
    if max_tgt_length == 0:
        max_tgt_length = 128
    if max_src_points == 0:
        max_src_points = 20
    if max_point_dim == 0:
        max_point_dim = 1

    if skipped_count > 0:
        logging.warning(f"Skipped {skipped_count} samples during processing")

    logging.info("Processing completed:")
    logging.info(f"  - Src vocabulary size: {len(src_vocab)}")
    logging.info(f"  - Tgt vocabulary size: {len(tgt_vocab)}")
    logging.info(f"  - Max target length: {max_tgt_length}")
    logging.info(f"  - Max source points: {max_src_points}")
    logging.info(f"  - Max point dimension: {max_point_dim}")

    return (
        src_vocab,
        tgt_vocab,
        max_tgt_length,
        max_src_points,
        max_point_dim,
        point_num_dist,
    )


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Calculate metadata from CSV file and output to YAML"
    )
    parser.add_argument(
        "-i", "--input", required=True, help="Input CSV file path"
    )
    parser.add_argument(
        "-o",
        "--output",
        help="Output YAML file path (default: input_file_metadata.yaml)",
    )
    parser.add_argument(
        "-v", "--verbose", action="store_true", help="Enable verbose logging"
    )
    parser.add_argument(
        "-p",
        "--parallel",
        action="store_true",
        help="Enable parallel processing",
    )
    parser.add_argument(
        "-w",
        "--workers",
        type=int,
        default=None,
        help="Number of worker processes (default: CPU count, max 8)",
    )
    parser.add_argument(
        "-c",
        "--chunk-size",
        type=int,
        default=1000,
        help="Chunk size for processing (default: 1000)",
    )

    args = parser.parse_args()

    # outputãƒ•ã‚¡ã‚¤ãƒ«åã®ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã‚’è¨­å®š
    if args.output is None:
        input_path = args.input
        if input_path.endswith(".csv"):
            args.output = input_path[:-4] + "_metadata.yaml"
        else:
            args.output = input_path + "_metadata.yaml"

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    try:
        print(f"ğŸ” Processing CSV file: {args.input}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®äº‹å‰ç¢ºèª
        file_size_mb = os.path.getsize(args.input) / (1024 * 1024)
        total_rows = sum(1 for _ in open(args.input)) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã

        print(f"ğŸ“Š File info: {file_size_mb:.2f} MB, {total_rows:,} rows")

        # ä¸¦åˆ—å‡¦ç†ã‹é †æ¬¡å‡¦ç†ã‹ã‚’é¸æŠ
        if args.parallel:
            print("\nâš¡ Starting parallel metadata calculation...")
            print(
                f"ğŸ”§ Workers: {args.workers or 'auto'}, Chunk size: {args.chunk_size}"
            )

            (
                src_vocab_set,
                tgt_vocab_set,
                max_tgt_length,
                max_src_points,
                max_point_dim,
                point_num_dist,
            ) = calculate_metadata_parallel(
                args.input, n_workers=args.workers, chunk_size=args.chunk_size
            )
        else:
            print("\nğŸš€ Starting single-pass metadata calculation...")

            (
                src_vocab_set,
                tgt_vocab_set,
                max_tgt_length,
                max_src_points,
                max_point_dim,
                point_num_dist,
            ) = calculate_all_metadata(args.input)

        print("\nğŸ”„ Converting vocabularies to sorted lists...")
        # ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
        src_vocab_list = sorted(list(src_vocab_set))
        tgt_vocab_list = sorted(list(tgt_vocab_set))

        # ç‰¹æ®Šãƒˆãƒ¼ã‚¯ãƒ³ã‚’å…ˆé ­ã«è¿½åŠ 
        src_vocab_list = ["[PAD]"] + src_vocab_list
        tgt_vocab_list = ["[PAD]", "[BOS]", "[EOS]"] + tgt_vocab_list

        # YAMLã«å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆ2ã¤ã®listãŒæœ€å¾Œã«ãªã‚‹ä¸¦ã³ï¼‰
        metadata = {
            "max_tgt_length": max_tgt_length,
            "max_src_points": max_src_points,
            "max_point_dim": max_point_dim,
            "src_vocab_list": src_vocab_list,
            "tgt_vocab_list": tgt_vocab_list,
            "point_num_dist": point_num_dist,
        }

        # YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
        print(f"ğŸ’¾ Writing metadata to: {args.output}")
        with open(args.output, "w", encoding="utf-8") as f:
            yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)

        # çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        print("\nâœ… Metadata calculation completed!")
        print("=" * 50)
        print(f"ğŸ“ Max target length:     {max_tgt_length:,}")
        print(f"ğŸ“Š Max source points:     {max_src_points:,}")
        print(f"ğŸ“ Max point dimension:   {max_point_dim:,}")
        print(f"ğŸ“š Source vocabulary size: {len(src_vocab_list):,}")
        print(f"ğŸ¯ Target vocabulary size: {len(tgt_vocab_list):,}")
        print(
            f"ğŸ“ˆ Point count distribution: {len(point_num_dist)} unique counts"
        )
        if point_num_dist:
            sorted_dist = sorted(point_num_dist.items())
            min_points, max_points = sorted_dist[0][0], sorted_dist[-1][0]
            total_samples = sum(
                len(indices) for indices in point_num_dist.values()
            )
            print(
                f"   Range: {min_points} to {max_points} points ({total_samples:,} total samples)"
            )
        print("=" * 50)
        print(f"ğŸ’¾ Results saved to: {args.output}")

    except Exception as e:
        logging.error(f"âŒ Error processing file: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
