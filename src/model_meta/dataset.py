import argparse
import ast
import math
import pickle
import random
from collections import Counter, defaultdict
from pathlib import Path
from typing import Dict, List, Optional

import lightning as pl
import torch
import yaml
from datasets import load_dataset
from torch.utils.data import DataLoader, Dataset, Sampler, random_split
from tqdm import tqdm


class TensorDataset(Dataset):
    """
    PyTorch DataLoaderã§ä½¿ç”¨å¯èƒ½ãªDatasetã‚¯ãƒ©ã‚¹
    ã‚ªãƒ³ãƒ‡ãƒžãƒ³ãƒ‰ã§tensorã«å¤‰æ›
    """

    def __init__(self, dataset):
        self.dataset = dataset

    def __len__(self):
        return len(self.dataset)

    def __getitem__(self, idx):
        sample = self.dataset[idx]
        try:
            return {
                "source": torch.tensor(
                    ast.literal_eval(sample["source"]), dtype=torch.long
                ),
                "target": torch.tensor(
                    ast.literal_eval(sample["target"]), dtype=torch.long
                ),
            }
        except (ValueError, SyntaxError):
            return {
                "source": torch.tensor([[0]], dtype=torch.long),
                "target": torch.tensor([0], dtype=torch.long),
            }


class LengthAwareTokenBatchSampler(Sampler):
    """
    åŒã˜é•·ã•ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã—ã€ã‹ã¤ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶ç´„ã‚’æº€ãŸã™ã‚µãƒ³ãƒ—ãƒ©ãƒ¼
    1. åŒã˜é•·ã•ã®ã‚µãƒ³ãƒ—ãƒ«ã‚’å„ªå…ˆçš„ã«ãƒãƒƒãƒã«å«ã‚ã‚‹
    2. æŒ‡å®šã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ä»¥ä¸Šã«ãªã‚‹ã¾ã§ã‚µãƒ³ãƒ—ãƒ«ã‚’è¿½åŠ 
    3. æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºã®åˆ¶é™ã‚‚è€ƒæ…®
    """

    def __init__(
        self,
        dataset: Dataset,
        min_tokens_per_batch: int,
        max_batch_size: int = 32,
        shuffle: bool = True,
        metadata_path: Optional[str] = None,
    ):
        self.dataset = dataset
        self.min_tokens_per_batch = min_tokens_per_batch
        self.max_batch_size = max_batch_size
        self.shuffle = shuffle
        self.metadata_path = metadata_path

        self._create_length_groups()

    def _create_length_groups(self):
        """ã‚½ãƒ¼ã‚¹é•·ã•ã«åŸºã¥ã„ã¦ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚°ãƒ«ãƒ¼ãƒ—åŒ–"""
        print("Creating length-based groups...")

        # metadata.yamlã‹ã‚‰point_num_distã‚’èª­ã¿è¾¼ã‚€
        if self.metadata_path and Path(self.metadata_path).exists():
            print(f"Loading point_num_dist from: {self.metadata_path}")
            with open(self.metadata_path, "r") as f:
                metadata = yaml.safe_load(f)
                point_num_dist = metadata.get("point_num_dist", {})
            print(f"finished loading point_num_dist from: {self.metadata_path}")

            original_to_subset = {}
            for subset_idx, orig_idx in tqdm(
                enumerate(self.dataset.indices),
                desc="Creating orig to subset idx mapping",
            ):
                original_to_subset[orig_idx] = subset_idx

            self.tokens_groups = defaultdict(list)
            for tokens, original_indices in tqdm(
                point_num_dist.items(),
                desc="Mapping point_num_dist to subset indices",
            ):
                for orig_idx in original_indices:
                    if orig_idx not in original_to_subset:
                        continue
                    self.tokens_groups[tokens].append(
                        original_to_subset[orig_idx]
                    )

            print(f"Mapped to {len(self.tokens_groups)} groups for Subset:")
            for tokens, indices in list(self.tokens_groups.items())[:5]:
                print(
                    f"  Length {tokens}: {len(indices)} samples (subset indices: {indices[:10]}{'...' if len(indices) > 10 else ''})"
                )

        else:
            print("No metadata file provided, creating groups dynamically...")
            self._create_groups_dynamically()

    def _create_groups_dynamically(self):
        """å‹•çš„ã«ã‚°ãƒ«ãƒ¼ãƒ—ã‚’ä½œæˆï¼ˆãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãŒãªã„å ´åˆã®ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ï¼‰"""

        print(f"Created {len(self.tokens_groups)} groups dynamically:")
        for length, indices in list(self.tokens_groups.items())[:5]:
            print(f"  Length {length}: {len(indices)} samples")

    def __iter__(self):
        """é•·ã•ãƒ™ãƒ¼ã‚¹ã®ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ã¨ãƒˆãƒ¼ã‚¯ãƒ³æ•°åˆ¶ç´„ã‚’çµ„ã¿åˆã‚ã›ãŸãƒãƒƒãƒç”Ÿæˆ"""

        # å„ã‚°ãƒ«ãƒ¼ãƒ—å†…ã§ã‚µãƒ³ãƒ—ãƒ«ã‚’ã‚·ãƒ£ãƒƒãƒ•ãƒ«
        for _, samples in self.tokens_groups.items():
            random.shuffle(samples)
        self.token_groups = sorted(
            self.tokens_groups.items(), key=lambda x: x[0]
        )

        # batchã®sourceãŒself.min_token_per_batchä»¥ä¸‹ã®ãƒˆãƒ¼ã‚¯ãƒ³æ•°ã«ãªã‚‹ã‚ˆã†ã«
        batch = []
        for tokens, indices in self.token_groups:
            for idx in indices:
                if (
                    len(batch) >= self.max_batch_size
                    or (len(batch) + 1) * tokens > self.min_tokens_per_batch
                ):
                    yield batch
                    batch = []
                batch.append(idx)
        if batch:
            yield batch

    def __len__(self):
        # æ¦‚ç®—ã®ãƒãƒƒãƒæ•°
        total_tokens = sum(
            key * len(value) for key, value in self.token_groups.items()
        )
        estimated_batches = max(1, total_tokens // self.min_tokens_per_batch)
        return estimated_batches


class CSVDataModule(pl.LightningDataModule):
    """
    æ±Žç”¨çš„ãªCSVãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆç”¨ã®LightningDataModule
    source/targetã‚«ãƒ©ãƒ ã‚’æŒã¤CSVãƒ•ã‚¡ã‚¤ãƒ«ã«å¯¾å¿œ
    """

    def __init__(
        self,
        data_path: str,
        batch_size: int = 32,
        num_workers: int = 4,
        train_val_split: float = 0.9,
        seed: int = 42,
        collate_fn: Optional[callable] = None,
        # ãƒãƒƒãƒãƒ³ã‚°æˆ¦ç•¥ã®é¸æŠž
        batching_strategy: str = "default",  # "default", "length_aware_token"
        min_tokens_per_batch: Optional[int] = None,
        max_batch_size: int = 32,
        metadata_path: Optional[str] = None,
    ):
        super().__init__()
        self.data_path = data_path
        self.batch_size = batch_size
        self.num_workers = num_workers
        self.train_val_split = train_val_split
        self.seed = seed
        self.collate_fn = collate_fn

        # ãƒãƒƒãƒãƒ³ã‚°æˆ¦ç•¥ã®è¨­å®š
        self.batching_strategy = batching_strategy
        self.min_tokens_per_batch = min_tokens_per_batch
        self.max_batch_size = max_batch_size
        self.metadata_path = metadata_path

        # ãƒãƒªãƒ‡ãƒ¼ã‚·ãƒ§ãƒ³
        if (
            batching_strategy == "length_aware_token"
            and min_tokens_per_batch is None
        ):
            raise ValueError(
                "min_tokens_per_batch must be specified when using length_aware_token strategy"
            )

        # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã‚’ä¿å­˜ã™ã‚‹å¤‰æ•°
        self.dataset = None
        self.train_dataset = None
        self.val_dataset = None
        self.test_dataset = None

    def setup(self, stage: Optional[str] = None):
        """
        ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆã®æº–å‚™
        stage: 'fit', 'validate', 'test', 'predict'ã®ã„ãšã‚Œã‹
        """
        if self.dataset is None:
            print(f"Loading dataset from: {self.data_path}")

            # HuggingFace datasetsã§CSVã‚’ãƒ­ãƒ¼ãƒ‰
            raw_dataset = load_dataset(
                "csv",
                data_files={"raw": self.data_path},
                streaming=False,
                split="raw",
            )
            # TensorDatasetã§ãƒ©ãƒƒãƒ—
            self.dataset = TensorDataset(raw_dataset)
            print(f"Dataset loaded: {len(self.dataset)} samples")

        # å­¦ç¿’ãƒ»æ¤œè¨¼ç”¨ã®åˆ†å‰²
        if stage == "fit" or stage is None:
            if self.train_dataset is None or self.val_dataset is None:
                train_size = int(self.train_val_split * len(self.dataset))
                val_size = len(self.dataset) - train_size

                self.train_dataset, self.val_dataset = random_split(
                    self.dataset,
                    [train_size, val_size],
                    generator=torch.Generator().manual_seed(self.seed),
                )

                print(f"Train dataset: {len(self.train_dataset)} samples")
                print(f"Validation dataset: {len(self.val_dataset)} samples")

        # ãƒ†ã‚¹ãƒˆç”¨ï¼ˆä»Šå›žã¯æ¤œè¨¼ã‚»ãƒƒãƒˆã¨åŒã˜ã‚‚ã®ã‚’ä½¿ç”¨ï¼‰
        if stage == "test":
            if self.test_dataset is None:
                self.test_dataset = self.val_dataset

    def train_dataloader(self):
        """å­¦ç¿’ç”¨DataLoader"""
        if self.batching_strategy == "length_aware_token":
            batch_sampler = LengthAwareTokenBatchSampler(
                dataset=self.train_dataset,
                min_tokens_per_batch=self.min_tokens_per_batch,
                max_batch_size=self.max_batch_size,
                shuffle=True,
                metadata_path=self.metadata_path,
            )
            return DataLoader(
                self.train_dataset,
                batch_sampler=batch_sampler,
                num_workers=self.num_workers,
                collate_fn=self.collate_fn,
                pin_memory=True,
            )
        else:
            # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å›ºå®šãƒãƒƒãƒã‚µã‚¤ã‚º
            return DataLoader(
                self.train_dataset,
                batch_size=self.batch_size,
                shuffle=True,
                num_workers=self.num_workers,
                collate_fn=self.collate_fn,
                pin_memory=True,
            )

    def val_dataloader(self):
        """æ¤œè¨¼ç”¨DataLoader"""
        return DataLoader(
            self.val_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            pin_memory=True,
        )

    def test_dataloader(self):
        """ãƒ†ã‚¹ãƒˆç”¨DataLoader"""
        return DataLoader(
            self.test_dataset,
            batch_size=self.batch_size,
            shuffle=False,
            num_workers=self.num_workers,
            collate_fn=self.collate_fn,
            pin_memory=True,
        )

    def save_pickle(self, file_path: str):
        """
        Save the CSVDataModule instance to a pickle file.

        Args:
            file_path: Path to save the pickle file
        """
        pickle_path = Path(file_path)
        pickle_path.parent.mkdir(parents=True, exist_ok=True)

        with open(pickle_path, "wb") as f:
            pickle.dump(self, f)

        print(f"CSVDataModule saved to {pickle_path}")

    @classmethod
    def load_pickle(cls, file_path: str) -> "CSVDataModule":
        """
        Load a CSVDataModule instance from a pickle file.

        Args:
            file_path: Path to the pickle file

        Returns:
            Loaded CSVDataModule instance
        """
        with open(file_path, "rb") as f:
            module = pickle.load(f)

        print(f"CSVDataModule loaded from {file_path}")
        return module
        print(f"Dataset saved as pickle: {file_path}")


def custom_collate_fn(batch):
    """
    å¯å¤‰é•·ãƒ†ãƒ³ã‚½ãƒ«ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã™ã‚‹collateé–¢æ•°
    """
    sources = [item["source"] for item in batch]
    targets = [item["target"] for item in batch]

    # sourceã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆ2D -> 3Dï¼‰
    max_seq_len = max(s.size(0) for s in sources)
    max_dim = max(s.size(1) for s in sources)

    padded_sources = torch.zeros(
        len(sources), max_seq_len, max_dim, dtype=torch.long
    )
    for i, src in enumerate(sources):
        seq_len, dim = src.shape
        padded_sources[i, :seq_len, :dim] = src

    # targetã®ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ï¼ˆ1D -> 2Dï¼‰
    max_target_len = max(t.size(0) for t in targets)
    padded_targets = torch.zeros(len(targets), max_target_len, dtype=torch.long)
    for i, tgt in enumerate(targets):
        target_len = tgt.shape[0]
        padded_targets[i, :target_len] = tgt

    return {"source": padded_sources, "target": padded_targets}


def analyze_batch_variance(dataloader, num_batches=5):
    """
    ãƒãƒƒãƒå†…ã®ã‚½ãƒ¼ã‚¹é•·ã•ã®åˆ†æ•£ã‚’åˆ†æžã™ã‚‹é–¢æ•°
    """
    variances = []
    batch_info = []

    for i, batch in enumerate(dataloader):
        if i >= num_batches:
            break

        source_lengths = [src.size(0) for src in batch["source"]]

        # åˆ†æ•£ã®è¨ˆç®—ï¼ˆå˜ä¸€å€¤ã®å ´åˆã¯0ã¨ã™ã‚‹ï¼‰
        if len(source_lengths) > 1:
            variance = torch.var(
                torch.tensor(source_lengths, dtype=torch.float)
            ).item()
        else:
            variance = 0.0

        variances.append(variance)

        batch_info.append(
            {
                "batch_id": i + 1,
                "batch_size": len(source_lengths),
                "lengths": source_lengths,
                "variance": variance,
                "min_length": min(source_lengths),
                "max_length": max(source_lengths),
                "mean_length": sum(source_lengths) / len(source_lengths),
            }
        )

    # NaNã‚’é™¤å¤–ã—ã¦å¹³å‡ã‚’è¨ˆç®—
    valid_variances = [v for v in variances if not (v != v)]  # NaNã§ãªã„å€¤ã®ã¿
    avg_variance = (
        sum(valid_variances) / len(valid_variances) if valid_variances else 0
    )

    return {"average_variance": avg_variance, "batch_details": batch_info}


# ä½¿ç”¨ä¾‹
if __name__ == "__main__":
    # Command line argument parsing
    parser = argparse.ArgumentParser(
        description="CSVDataModule - Load CSV data or save as pickle"
    )
    parser.add_argument(
        "--save-pickle",
        type=str,
        help="Save the CSVDataModule as a pickle file to the specified path",
    )
    parser.add_argument("--csv-path", type=str, help="Path to the CSV file")
    parser.add_argument(
        "--metadata-path", type=str, help="Path to the metadata file"
    )
    parser.add_argument(
        "--num-workers",
        type=int,
        default=0,
        help="Number of workers for data loading (default: 0)",
    )
    parser.add_argument(
        "--demo", action="store_true", help="Run the original demo/test"
    )
    parser.add_argument(
        "--batch-test", action="store_true", help="Run batching strategy tests"
    )

    args = parser.parse_args()

    if args.save_pickle:
        if not args.csv_path:
            print("Error: --csv-path is required when using --save-pickle")
            parser.print_help()
            exit(1)

        # Create and save CSVDataModule
        print(f"Creating CSVDataModule from: {args.csv_path}")
        if args.metadata_path:
            print(f"Using metadata from: {args.metadata_path}")

        data_module = CSVDataModule(
            data_path=args.csv_path,
            batch_size=32,
            num_workers=args.num_workers,
            train_val_split=0.9,
            collate_fn=custom_collate_fn,
            metadata_path=args.metadata_path,
        )

        # Setup to load the data
        data_module.setup()

        print(f"Saving CSVDataModule to: {args.save_pickle}")
        data_module.save_pickle(args.save_pickle)
        print("âœ… CSVDataModule saved successfully!")

        # Print some stats
        print("ðŸ“Š Dataset stats:")
        print(f"  - Total samples: {len(data_module.dataset)}")
        print(f"  - Train samples: {len(data_module.train_dataset)}")
        print(f"  - Val samples: {len(data_module.val_dataset)}")
        if data_module.test_dataset:
            print(f"  - Test samples: {len(data_module.test_dataset)}")

    elif args.demo:
        # Run the original demo/test code
        print("ðŸš€ Running CSVDataModule demo...")

        # Use default paths if not provided
        csv_path = args.csv_path or "experiment/column_format.csv"
        metadata_path = args.metadata_path or "test_metadata.yaml"

        # Test CSVDataModule
        data_module = CSVDataModule(
            data_path=csv_path,
            batch_size=32,
            num_workers=args.num_workers,
            train_val_split=0.9,
            collate_fn=custom_collate_fn,
            metadata_path=metadata_path,
        )

        print("Setting up data module...")
        data_module.setup()

        print(f"Dataset size: {len(data_module.dataset)}")
        print(f"Train size: {len(data_module.train_dataset)}")
        print(f"Val size: {len(data_module.val_dataset)}")
        print(f"Test size: {len(data_module.test_dataset)}")

        # Test train dataloader
        train_loader = data_module.train_dataloader()
        print(f"Train batches: {len(train_loader)}")

        # Get a sample batch
        batch = next(iter(train_loader))
        print(f"Sample batch keys: {batch.keys()}")
        for key, value in batch.items():
            if isinstance(value, torch.Tensor):
                print(f"  {key}: {value.shape}")
            else:
                print(f"  {key}: {type(value)}")

        print("âœ… Demo completed successfully!")

    elif args.batch_test:
        # Run batching strategy tests
        print("=== Testing Different Batching Strategies ===")

        data_path = "data/training/superfib_r1_dataset.csv"
        metadata_path = "data/training/superfib_r1_metadata.yaml"
        batch_size = 2

        # 1. ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆï¼ˆå›ºå®šãƒãƒƒãƒã‚µã‚¤ã‚ºï¼‰
        print("\n1. Default Fixed Batch Size")
        data_module_default = CSVDataModule(
            data_path=data_path,
            batch_size=batch_size,
            num_workers=0,  # ãƒ†ã‚¹ãƒˆç”¨ã«0ã«è¨­å®š
            train_val_split=0.8,
            collate_fn=custom_collate_fn,
            batching_strategy="default",
        )

        data_module_default.setup("fit")
        train_loader_default = data_module_default.train_dataloader()

        print(f"Train samples: {len(data_module_default.train_dataset)}")

        # 2. é•·ã•ã‚’è€ƒæ…®ã—ãŸãƒˆãƒ¼ã‚¯ãƒ³æ•°ãƒ™ãƒ¼ã‚¹ã®ãƒãƒƒãƒãƒ³ã‚°
        print("\n2. Length-Aware Token Count-based Batching")
        data_module_token = CSVDataModule(
            data_path=data_path,
            batch_size=batch_size,  # æœ€å¤§ãƒãƒƒãƒã‚µã‚¤ã‚ºã¨ã—ã¦æ©Ÿèƒ½
            num_workers=13,
            train_val_split=0.8,
            collate_fn=custom_collate_fn,
            batching_strategy="length_aware_token",
            min_tokens_per_batch=10000,  # æœ€å°ãƒˆãƒ¼ã‚¯ãƒ³æ•°
            max_batch_size=128,
            metadata_path=metadata_path,
        )

        data_module_token.setup("fit")
        print("===Train loader===")
        train_loader_token = data_module_token.train_dataloader()

        print("Batch info for length-aware token-based batching:")
        for i, batch in enumerate(train_loader_token):
            if i >= 10:
                break
            batch_size = len(batch["source"])
            source_lengths = [src.size(0) for src in batch["source"]]
            count_dict = dict(Counter(source_lengths))
            print(
                f"  Batch {i + 1}: size={batch_size}, total_tokens={sum(source_lengths)} "
                f"Histogram of source_lengths: {count_dict}",
            )

        print("===Val loader===")
        val_loader_token = data_module_token.val_dataloader()

        print("Batch info for length-aware token-based batching:")
        for i, batch in enumerate(val_loader_token):
            if i >= 10:
                break
            batch_size = len(batch["source"])
            source_lengths = [src.size(0) for src in batch["source"]]
            count_dict = dict(Counter(source_lengths))
            print(
                f"  Batch {i + 1}: size={batch_size}, total_tokens={sum(source_lengths)} "
                f"Histogram of source_lengths: {count_dict}",
            )

        print("âœ… Batch test completed successfully!")

    else:
        # Default behavior - show help
        parser.print_help()
        print("\nExamples:")
        print("  # Save CSVDataModule as pickle:")
        print(
            "  python -m src.model_meta.dataset --save-pickle my_datamodule.pkl --csv-path data.csv"
        )
        print(
            "  python -m src.model_meta.dataset --save-pickle output.pkl --csv-path data/my_data.csv --metadata-path metadata.yaml --num-workers 4"
        )
        print()
        print("  # Run demo:")
        print("  python -m src.model_meta.dataset --demo --csv-path data.csv")
        print()
        print("  # Run batching strategy tests:")
        print("  python -m src.model_meta.dataset --batch-test")
