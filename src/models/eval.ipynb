{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/takeruito/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python312.zip', '/home/takeruito/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12', '/home/takeruito/.local/share/uv/python/cpython-3.12.7-linux-x86_64-gnu/lib/python3.12/lib-dynload', '', '/home/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages', '/home/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/setuptools/_vendor', '/tmp/tmp4r6fj068', '/home/takeruito/work/PrfSR', '/home/takeruito/work/PrfSR', '/home/takeruito/work/PrfSR']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing tokenizers from vocabulary...\n",
      "Source vocabulary size: 1005\n",
      "Target vocabulary size: 13\n",
      "Source vocab sample: [' ', ',', '0', '1', '10', '100', '1000', '101', '102', '103']\n",
      "Target vocab sample: [' ', '(', ')', ',', '1', '2', '3', '4', 'C', 'P']\n",
      "Tokenizer initialized with 1009 total tokens\n",
      "Max token ID: 1008\n",
      "Tokenizer initialized with 17 total tokens\n",
      "Max token ID: 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from torch import nn\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from omegaconf import OmegaConf\n",
    "import lightning as L\n",
    "import numpy as np\n",
    "\n",
    "# Add safe globals for pathlib.PosixPath to handle checkpoint loading\n",
    "torch.serialization.add_safe_globals([Path])\n",
    "\n",
    "import sys\n",
    "sys.path.append(str(Path().cwd().parent.parent))\n",
    "print(sys.path)\n",
    "\n",
    "from src.models.data import get_tgt_str, id2token, TransformerDataset\n",
    "from src.models.train import LitTransformer\n",
    "\n",
    "from src.models.dataset import PRFDataset\n",
    "\n",
    "proj_root = Path().cwd().parent.parent\n",
    "\n",
    "cfg = OmegaConf.load(proj_root / \"models/first_superfib/config.yaml\")\n",
    "\n",
    "ckpt_path = proj_root / \"models/first_superfib/model-step=6500-val_loss=0.10.ckpt\"\n",
    "checkpoint = torch.load(ckpt_path, map_location=\"cpu\", weights_only=False)\n",
    "\n",
    "model = LitTransformer(**checkpoint[\"hyper_parameters\"])\n",
    "model.load_state_dict(checkpoint[\"state_dict\"], strict=False)\n",
    "device = \"cuda\"\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "csv_path = proj_root / \"data/training/superfib_r1.csv\"\n",
    "metadata_path = proj_root / \"data/training/superfib_r1_metadata.yaml\"\n",
    "\n",
    "metadata = OmegaConf.load(metadata_path)\n",
    "\n",
    "dataset = PRFDataset(\n",
    "    csv_path=csv_path,\n",
    "    max_tgt_length = metadata.max_tgt_length,\n",
    "    max_src_points= metadata.max_src_points,\n",
    "    src_vocab_list= metadata.src_vocab_list,\n",
    "    tgt_vocab_list= metadata.tgt_vocab_list,\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate whole string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   2 1007 1007    6    5    7    5  119    5  230]\n",
      "[2 0 0 0 0 0 0 0 0 0]\n",
      "[2 3 0 0 0 0 0 0 0 0]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "embedding(): argument 'indices' (position 2) must be Tensor, not numpy.ndarray",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 43\u001b[39m\n\u001b[32m     41\u001b[39m \u001b[38;5;28mprint\u001b[39m(arr[:\u001b[32m10\u001b[39m])\n\u001b[32m     42\u001b[39m tgt = torch.tensor(arr).to(device)\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m output = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# (T, N, C)\u001b[39;00m\n\u001b[32m     44\u001b[39m pred = output[read_token_id, \u001b[32m0\u001b[39m, :]\n\u001b[32m     45\u001b[39m max_values, max_id= torch.max(pred, axis=\u001b[32m0\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/src/models/train.py:97\u001b[39m, in \u001b[36mLitTransformer.forward\u001b[39m\u001b[34m(self, src, tgt)\u001b[39m\n\u001b[32m     91\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, src, tgt):\n\u001b[32m     92\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     93\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m     94\u001b[39m \u001b[33;03m    - src: Tensor of shape (N, S: seq len, Emb)\u001b[39;00m\n\u001b[32m     95\u001b[39m \u001b[33;03m    - tgt: Tensor of shape (N, T: seq len, Emb)\u001b[39;00m\n\u001b[32m     96\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m97\u001b[39m     src = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msrc_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43msrc\u001b[49m\u001b[43m)\u001b[49m * torch.sqrt(\n\u001b[32m     98\u001b[39m         torch.tensor(\u001b[38;5;28mself\u001b[39m.d_model, dtype=torch.float32)\n\u001b[32m     99\u001b[39m     )\n\u001b[32m    100\u001b[39m     tgt = \u001b[38;5;28mself\u001b[39m.tgt_embedding(tgt) * torch.sqrt(\n\u001b[32m    101\u001b[39m         torch.tensor(\u001b[38;5;28mself\u001b[39m.d_model, dtype=torch.float32)\n\u001b[32m    102\u001b[39m     )\n\u001b[32m    103\u001b[39m     src = src.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# (S, N, E)\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/sparse.py:190\u001b[39m, in \u001b[36mEmbedding.forward\u001b[39m\u001b[34m(self, input)\u001b[39m\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) -> Tensor:\n\u001b[32m--> \u001b[39m\u001b[32m190\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    191\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    192\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    193\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    194\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_norm\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    195\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mnorm_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    196\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    197\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    198\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/functional.py:2551\u001b[39m, in \u001b[36membedding\u001b[39m\u001b[34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[39m\n\u001b[32m   2545\u001b[39m     \u001b[38;5;66;03m# Note [embedding_renorm set_grad_enabled]\u001b[39;00m\n\u001b[32m   2546\u001b[39m     \u001b[38;5;66;03m# XXX: equivalent to\u001b[39;00m\n\u001b[32m   2547\u001b[39m     \u001b[38;5;66;03m# with torch.no_grad():\u001b[39;00m\n\u001b[32m   2548\u001b[39m     \u001b[38;5;66;03m#   torch.embedding_renorm_\u001b[39;00m\n\u001b[32m   2549\u001b[39m     \u001b[38;5;66;03m# remove once script supports set_grad_enabled\u001b[39;00m\n\u001b[32m   2550\u001b[39m     _no_grad_embedding_renorm_(weight, \u001b[38;5;28minput\u001b[39m, max_norm, norm_type)\n\u001b[32m-> \u001b[39m\u001b[32m2551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43membedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscale_grad_by_freq\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msparse\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mTypeError\u001b[39m: embedding(): argument 'indices' (position 2) must be Tensor, not numpy.ndarray"
     ]
    }
   ],
   "source": [
    "src_vocab = model.src_vocab\n",
    "tgt_vocab = model.tgt_vocab\n",
    "src_max_len = model.src_max_len\n",
    "tgt_max_len = model.tgt_max_len\n",
    "\n",
    "inputs = [(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (9,)]\n",
    "#inputs = [(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
    "#inputs = [(0,), (0,)]\n",
    "\n",
    "#outputs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "#outputs = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "#outputs = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "#outputs = [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "#outputs = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "#outputs = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#outputs = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "outputs = [0, 0, 2, 4, 6, 8, 10, 12, 14, 16]\n",
    "\n",
    "#outputs = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "#outputs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "#outputs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "#outputs = [0, 0]\n",
    "#outputs = [1, 1]\n",
    "#outputs = [2, 2]\n",
    "#outputs = [3, 3]\n",
    "\n",
    "\n",
    "\n",
    "tgt_vocab = dataset.get_tgt_tokenizer().vocab\n",
    "src_tokenizer = dataset.get_src_tokenizer()\n",
    "src = dataset._create_src_array(inputs, outputs)\n",
    "print(src[:10])\n",
    "arr = dataset._create_tgt_array(current_str)\n",
    "arr[arr == tgt_vocab[\"[EOS]\"]]= tgt_vocab[\"[PAD]\"]\n",
    "current_str = \"\"\n",
    "\n",
    "with torch.no_grad():\n",
    "    for read_token_id in range(tgt_max_len - 1):\n",
    "        arr = dataset._create_tgt_array(current_str)\n",
    "        print(arr[:10])\n",
    "        tgt = torch.tensor(arr).to(device)\n",
    "        output = model(src, tgt) # (T, N, C)\n",
    "        pred = output[read_token_id, 0, :]\n",
    "        max_values, max_id= torch.max(pred, axis=0)\n",
    "        new_token = tgt_vocab.id_to_token(max_id)\n",
    "        print(f\"new_token: {new_token}\")\n",
    "        current_str += new_token\n",
    "        if max_id == tgt_vocab[\"[EOS]\"]:\n",
    "            break\n",
    "\n",
    "list_letters = [id2token(tgt_vocab, id) for id in current_str]\n",
    "print(list_letters)\n",
    "print(str(\"\".join(list_letters[1:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"/home/takeru/AlphaSymbol/data/prfndim/d3-a2-c3-r3-status.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "dataset = TransformerDataset(df)\n",
    "\n",
    "idx = 0\n",
    "src, tgt, tgt_correct = dataset[idx][0], dataset[idx][1][:-1], dataset[idx][1][1:]\n",
    "src, tgt, tgt_correct = src.to(device), tgt.to(device), tgt_correct.to(device)\n",
    "pred_token_place= 3 #>=1\n",
    "\n",
    "print(\"=== Input ===\")\n",
    "input_str = dataset.df[\"expr\"].iloc[idx]    \n",
    "print(\"input string: \", input_str)\n",
    "print(\"pred_token_place: \", pred_token_place)\n",
    "\n",
    "print()\n",
    "print(\"=== Raw Data ===\")\n",
    "print(\"src: \", src)\n",
    "print(\"tgt: \", tgt)\n",
    "print(\"tgt_correct: \", tgt_correct)\n",
    "\n",
    "src = src.reshape((1, -1))\n",
    "pad_tensor = torch.tensor([tgt_vocab[\"<pad>\"] for _ in range(len(tgt) - pred_token_place)]).to(device)\n",
    "tgt = torch.cat((tgt[:pred_token_place], pad_tensor)).reshape((1, -1))\n",
    "tgt_correct = tgt_correct[pred_token_place- 1].reshape((1, ))\n",
    "\n",
    "print()\n",
    "print(\"=== Processed data for inference ===\")\n",
    "print(\"src: \", src)\n",
    "print(\"tgt: \", tgt)\n",
    "print(\"tgt_correct: \", tgt_correct)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"=== Prediction ===\")\n",
    "model.eval()\n",
    "output = model(src, tgt) # (Seq, N, E)\n",
    "pred_token = output[pred_token_place- 1, 0, :]\n",
    "print(\"pred: \", pred_token)\n",
    "token_id =  torch.argmax(pred_token).item()\n",
    "print(\"token_id: \",token_id)\n",
    "print(\"token: \", id2token(tgt_vocab, token_id))\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<pad>\"])\n",
    "pred_for_loss = pred_token.reshape((1, -1))\n",
    "loss = loss_fn(pred_for_loss, tgt_correct)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate from src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.tensor([[1, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4,\n",
    "         7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4,\n",
    "         7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
    "\n",
    "tgt = torch.tensor([[1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "       device='cuda:0')\n",
    "\n",
    "output = model(src, tgt)\n",
    "print(output[1, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate loss along with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "src, tgt, tgt_correct = dataset[idx][0], dataset[idx][1][:-1], dataset[idx][1][1:]\n",
    "src = src.reshape((1, -1)).to(device)\n",
    "tgt = tgt.reshape((1, -1)).to(device)\n",
    "tgt_correct = tgt_correct.reshape((1, -1)).to(device)\n",
    "output = model(src, tgt) # (T, N=1, C)\n",
    "output = output.permute(1, 2, 0) # (N=1, C, T)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<pad>\"])\n",
    "loss = loss_fn(output, tgt_correct)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
