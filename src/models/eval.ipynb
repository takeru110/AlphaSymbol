{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train import LitTransformer\n",
    "from pathlib import Path\n",
    "from data import TransformerDataset\n",
    "import torch\n",
    "import yaml\n",
    "from data import get_tgt_str, id2token\n",
    "import pandas as pd\n",
    "\n",
    "from torch import nn\n",
    "import pandas as pd\n",
    "from data import TransformerDataset\n",
    "\n",
    "model_path = \"/home/takeru/AlphaSymbol/models/d3-a5-c3-r5/2024-1218-0225-50/lightning_logs/version_0/checkpoints/epoch=633-step=22190.ckpt\"\n",
    "\n",
    "model = LitTransformer.load_from_checkpoint(model_path)\n",
    "device = model.device\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate whole string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "src_vocab = model.src_vocab\n",
    "tgt_vocab = model.tgt_vocab\n",
    "src_max_len = model.src_max_len\n",
    "tgt_max_len = model.tgt_max_len\n",
    "\n",
    "inputs = [(0,), (1,), (2,), (3,), (4,), (5,), (6,), (7,), (8,), (9,)]\n",
    "#inputs = [(0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,), (0,)]\n",
    "#inputs = [(0,), (0,)]\n",
    "\n",
    "#outputs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "#outputs = [2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
    "#outputs = [3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "#outputs = [4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
    "#outputs = [6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16]\n",
    "#outputs = [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "#outputs = [0, 2, 4, 6, 8, 10, 12, 14, 16, 18]\n",
    "outputs = [0, 0, 2, 4, 6, 8, 10, 12, 14, 16]\n",
    "\n",
    "#outputs = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
    "#outputs = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
    "#outputs = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]\n",
    "#outputs = [0, 0]\n",
    "#outputs = [1, 1]\n",
    "#outputs = [2, 2]\n",
    "#outputs = [3, 3]\n",
    "\n",
    "\n",
    "src_str = get_tgt_str(inputs, outputs)\n",
    "src_ends = torch.tensor([src_vocab[\"<sos>\"]] + [src_vocab[i] for i in src_str] + [src_vocab[\"<eos>\"]])\n",
    "src_padded = torch.cat([src_ends, torch.tensor([src_vocab[\"<pad>\"] for _ in range(src_max_len - src_ends.shape[0])])])\n",
    "src = src_padded.reshape((1, -1)).to(device)\n",
    "\n",
    "current_str = [tgt_vocab[\"<sos>\"]]\n",
    "\n",
    "with torch.no_grad():\n",
    "    for read_token_id in range(tgt_max_len - 1):\n",
    "        tgt_str = current_str + [tgt_vocab[\"<pad>\"]] * (tgt_max_len - len(current_str) - 1)\n",
    "        tgt= torch.tensor(tgt_str).to(device)\n",
    "        tgt = tgt.reshape((1, -1))\n",
    "        output = model(src, tgt) # (T, N, C)\n",
    "        pred = output[read_token_id, 0, :]\n",
    "        max_values, max_id= torch.max(pred, axis=0)\n",
    "        current_str.append(max_id.item())\n",
    "        if max_id == tgt_vocab[\"<eos>\"]:\n",
    "            break\n",
    "\n",
    "list_letters = [id2token(tgt_vocab, id) for id in current_str]\n",
    "print(list_letters)\n",
    "print(str(\"\".join(list_letters[1:-1])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate each token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "data_path = \"/home/takeru/AlphaSymbol/data/prfndim/d3-a2-c3-r3-status.csv\"\n",
    "df = pd.read_csv(data_path)\n",
    "dataset = TransformerDataset(df)\n",
    "\n",
    "idx = 0\n",
    "src, tgt, tgt_correct = dataset[idx][0], dataset[idx][1][:-1], dataset[idx][1][1:]\n",
    "src, tgt, tgt_correct = src.to(device), tgt.to(device), tgt_correct.to(device)\n",
    "pred_token_place= 3 #>=1\n",
    "\n",
    "print(\"=== Input ===\")\n",
    "input_str = dataset.df[\"expr\"].iloc[idx]    \n",
    "print(\"input string: \", input_str)\n",
    "print(\"pred_token_place: \", pred_token_place)\n",
    "\n",
    "print()\n",
    "print(\"=== Raw Data ===\")\n",
    "print(\"src: \", src)\n",
    "print(\"tgt: \", tgt)\n",
    "print(\"tgt_correct: \", tgt_correct)\n",
    "\n",
    "src = src.reshape((1, -1))\n",
    "pad_tensor = torch.tensor([tgt_vocab[\"<pad>\"] for _ in range(len(tgt) - pred_token_place)]).to(device)\n",
    "tgt = torch.cat((tgt[:pred_token_place], pad_tensor)).reshape((1, -1))\n",
    "tgt_correct = tgt_correct[pred_token_place- 1].reshape((1, ))\n",
    "\n",
    "print()\n",
    "print(\"=== Processed data for inference ===\")\n",
    "print(\"src: \", src)\n",
    "print(\"tgt: \", tgt)\n",
    "print(\"tgt_correct: \", tgt_correct)\n",
    "\n",
    "\n",
    "print()\n",
    "print(\"=== Prediction ===\")\n",
    "model.eval()\n",
    "output = model(src, tgt) # (Seq, N, E)\n",
    "pred_token = output[pred_token_place- 1, 0, :]\n",
    "print(\"pred: \", pred_token)\n",
    "token_id =  torch.argmax(pred_token).item()\n",
    "print(\"token_id: \",token_id)\n",
    "print(\"token: \", id2token(tgt_vocab, token_id))\n",
    "\n",
    "\n",
    "    \n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<pad>\"])\n",
    "pred_for_loss = pred_token.reshape((1, -1))\n",
    "loss = loss_fn(pred_for_loss, tgt_correct)\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate from src, tgt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "src = torch.tensor([[1, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4,\n",
    "         7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4,\n",
    "         7, 3, 4, 5, 6, 4, 7, 3, 4, 5, 6, 4, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]], device='cuda:0')\n",
    "\n",
    "tgt = torch.tensor([[1, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
    "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]],\n",
    "       device='cuda:0')\n",
    "\n",
    "output = model(src, tgt)\n",
    "print(output[1, 0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate loss along with Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = 0\n",
    "src, tgt, tgt_correct = dataset[idx][0], dataset[idx][1][:-1], dataset[idx][1][1:]\n",
    "src = src.reshape((1, -1)).to(device)\n",
    "tgt = tgt.reshape((1, -1)).to(device)\n",
    "tgt_correct = tgt_correct.reshape((1, -1)).to(device)\n",
    "output = model(src, tgt) # (T, N=1, C)\n",
    "output = output.permute(1, 2, 0) # (N=1, C, T)\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=tgt_vocab[\"<pad>\"])\n",
    "loss = loss_fn(output, tgt_correct)\n",
    "print(loss)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.12.7",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
