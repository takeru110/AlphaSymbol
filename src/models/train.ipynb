{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9c7f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OmegaConf imported successfully\n",
      "üìÅ Root path: /Users/takeruito/work/PrfSR\n",
      "üöÄ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# OmegaConf for configuration management (Hydra-style)\n",
    "try:\n",
    "    from omegaconf import OmegaConf\n",
    "    print(\"‚úÖ OmegaConf imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå OmegaConf not found. Install with: pip install omegaconf\")\n",
    "    raise\n",
    "\n",
    "root_path = Path.cwd().parent.parent\n",
    "print(\"üìÅ Root path:\", root_path)\n",
    "sys.path.append(str(root_path))\n",
    "\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "from src.models.dataset import CSVDataModule, custom_collate_fn\n",
    "from src.models.models import LitTransformer\n",
    "\n",
    "print(\"üöÄ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0bea7",
   "metadata": {},
   "source": [
    "# Load Config path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d863d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration loaded with OmegaConf:\n",
      "Metadata path: /Users/takeruito/work/PrfSR/data/training/superfib_r1_metadata.yaml\n",
      "Data path: /Users/takeruito/work/PrfSR/data/training/superfib_r1_dataset.csv\n",
      "max_epoch: 1\n",
      "max_value: 2000\n",
      "min_n_tokens_in_batch: 200\n",
      "test_ratio: 0.01\n",
      "val_ratio: 0.25\n",
      "num_workers: 13\n",
      "token_embed_dim: 16\n",
      "emb_expansion_factor: 1\n",
      "learning_rate: 3*10**(-4)\n",
      "batch_size: 2048\n",
      "batching_strategy: length_aware_token\n",
      "\n",
      "ü§ñ Transformer config:\n",
      "nhead: 16\n",
      "num_encoder_layers: 6\n",
      "num_decoder_layers: 6\n",
      "dim_feedforward: 512\n",
      "dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Load config using OmegaConf (Hydra-style)\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "train_config_path = root_path / \"src/models/training_config.yaml\"\n",
    "assert Path(train_config_path).exists(), FileNotFoundError(f\"Train config file not found: {train_config_path}\")\n",
    "\n",
    "# Load with OmegaConf for dot notation access\n",
    "config = OmegaConf.load(train_config_path)\n",
    "\n",
    "print(f\"üîß Configuration loaded with OmegaConf:\")\n",
    "print(f\"Metadata path: {config.metadata_path}\")\n",
    "print(f\"Data path: {config.data_path}\")\n",
    "print(f\"max_epoch: {config.max_epoch}\")\n",
    "print(f\"max_value: {config.max_value}\")\n",
    "print(f\"min_n_tokens_in_batch: {config.min_n_tokens_in_batch}\")\n",
    "print(f\"test_ratio: {config.test_ratio}\")\n",
    "print(f\"val_ratio: {config.val_ratio}\")\n",
    "print(f\"num_workers: {config.num_workers}\")\n",
    "print(f\"token_embed_dim: {config.token_embed_dim}\")\n",
    "print(f\"emb_expansion_factor: {config.emb_expansion_factor}\")\n",
    "print(f\"learning_rate: {config.learning_rate}\")\n",
    "print(f\"batch_size: {config.batch_size}\")\n",
    "print(f\"batching_strategy: {config.batching_strategy}\")\n",
    "\n",
    "print(f\"\\nü§ñ Transformer config:\")\n",
    "print(f\"nhead: {config.transformer.nhead}\")\n",
    "print(f\"num_encoder_layers: {config.transformer.num_encoder_layers}\")\n",
    "print(f\"num_decoder_layers: {config.transformer.num_decoder_layers}\")\n",
    "print(f\"dim_feedforward: {config.transformer.dim_feedforward}\")\n",
    "print(f\"dropout: {config.transformer.dropout}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd671109",
   "metadata": {},
   "source": [
    "# Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "23ca7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Metadata loaded:\n",
      "max_src_points: 80\n",
      "max_tgt_length: 859\n",
      "max_point_dim: 4\n",
      "src_vocab_size: 1002\n",
      "tgt_vocab_size: 15\n",
      "\n",
      "üî§ Source vocabulary (first 10): ['[PAD]', '0', '1', '10', '100', '1000', '101', '102', '103', '104']\n",
      "üéØ Target vocabulary (first 10): ['[PAD]', '[BOS]', '[EOS]', '(', ')', ',', '1', '2', '3', '4']\n",
      "\n",
      "‚úÖ Inverse vocabularies created:\n",
      "src '[PAD]' index: 0\n",
      "tgt '[PAD]' index: 0\n",
      "tgt '[BOS]' index: 1\n",
      "tgt '[EOS]' index: 2\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "\n",
    "# Load metadata using OmegaConf\n",
    "metadata_path = config.metadata_path\n",
    "metadata = OmegaConf.load(metadata_path)\n",
    "\n",
    "print(f\"üìä Metadata loaded:\")\n",
    "print(f\"max_src_points: {metadata.max_src_points}\")\n",
    "print(f\"max_tgt_length: {metadata.max_tgt_length}\")\n",
    "print(f\"max_point_dim: {metadata.max_point_dim}\")\n",
    "print(f\"src_vocab_size: {len(metadata.src_vocab_list)}\")\n",
    "print(f\"tgt_vocab_size: {len(metadata.tgt_vocab_list)}\")\n",
    "\n",
    "# Create inverse vocabularies more efficiently\n",
    "src_inv_vocab = {token: idx for idx, token in enumerate(metadata.src_vocab_list)}\n",
    "tgt_inv_vocab = {token: idx for idx, token in enumerate(metadata.tgt_vocab_list)}\n",
    "\n",
    "print(f\"\\nüî§ Source vocabulary (first 10): {list(src_inv_vocab.keys())[:10]}\")\n",
    "print(f\"üéØ Target vocabulary (first 10): {list(tgt_inv_vocab.keys())[:10]}\")\n",
    "\n",
    "# Add inverse vocabularies to metadata\n",
    "metadata.src_inv_vocab = src_inv_vocab\n",
    "metadata.tgt_inv_vocab = tgt_inv_vocab\n",
    "\n",
    "print(f\"\\n‚úÖ Inverse vocabularies created:\")\n",
    "print(f\"src '[PAD]' index: {metadata.src_inv_vocab['[PAD]']}\")\n",
    "print(f\"tgt '[PAD]' index: {metadata.tgt_inv_vocab['[PAD]']}\")\n",
    "print(f\"tgt '[BOS]' index: {metadata.tgt_inv_vocab['[BOS]']}\")\n",
    "print(f\"tgt '[EOS]' index: {metadata.tgt_inv_vocab['[EOS]']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30415c87",
   "metadata": {},
   "source": [
    "# Load CSV by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9dce438",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtakeit\u001b[0m (\u001b[33mtakeit-Keio University Global Page\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/Users/takeruito/work/PrfSR/src/models/wandb/run-20250707_030109-tobnhzq7</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/takeit-Keio%20University%20Global%20Page/prfsr-chunked-training/runs/tobnhzq7' target=\"_blank\">chunk_training_length_aware_token</a></strong> to <a href='https://wandb.ai/takeit-Keio%20University%20Global%20Page/prfsr-chunked-training' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/takeit-Keio%20University%20Global%20Page/prfsr-chunked-training' target=\"_blank\">https://wandb.ai/takeit-Keio%20University%20Global%20Page/prfsr-chunked-training</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/takeit-Keio%20University%20Global%20Page/prfsr-chunked-training/runs/tobnhzq7' target=\"_blank\">https://wandb.ai/takeit-Keio%20University%20Global%20Page/prfsr-chunked-training/runs/tobnhzq7</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• WandB initialized successfully!\n",
      "üìä Project: prfsr-chunked-training\n",
      "üè∑Ô∏è Run name: chunk_training_length_aware_token\n"
     ]
    }
   ],
   "source": [
    "# Initialize WandB\n",
    "import wandb\n",
    "\n",
    "# WandB project configuration\n",
    "wandb_config = {\n",
    "    \"project\": \"prfsr-chunked-training\",\n",
    "    \"name\": f\"chunk_training_{config.batching_strategy}\",\n",
    "    \"config\": {\n",
    "        \"batch_size\": config.batch_size,\n",
    "        \"learning_rate\": eval(config.learning_rate),\n",
    "        \"max_epoch\": config.max_epoch,\n",
    "        \"token_embed_dim\": config.token_embed_dim,\n",
    "        \"emb_expansion_factor\": config.emb_expansion_factor,\n",
    "        \"batching_strategy\": config.batching_strategy,\n",
    "        \"min_tokens_per_batch\": config.min_n_tokens_in_batch,\n",
    "        \"transformer_nhead\": config.transformer.nhead,\n",
    "        \"transformer_num_encoder_layers\": config.transformer.num_encoder_layers,\n",
    "        \"transformer_num_decoder_layers\": config.transformer.num_decoder_layers,\n",
    "        \"transformer_dim_feedforward\": config.transformer.dim_feedforward,\n",
    "        \"transformer_dropout\": config.transformer.dropout,\n",
    "        \"max_src_points\": metadata.max_src_points,\n",
    "        \"max_tgt_length\": metadata.max_tgt_length,\n",
    "        \"src_vocab_size\": len(metadata.src_vocab_list),\n",
    "        \"tgt_vocab_size\": len(metadata.tgt_vocab_list),\n",
    "    }\n",
    "}\n",
    "\n",
    "# Initialize WandB run\n",
    "wandb.init(**wandb_config)\n",
    "\n",
    "print(\"üî• WandB initialized successfully!\")\n",
    "print(f\"üìä Project: {wandb_config['project']}\")\n",
    "print(f\"üè∑Ô∏è Run name: {wandb_config['name']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/loggers/wandb.py:396: UserWarning: There is a wandb run already in progress and newly created instances of `WandbLogger` will reuse this run. If this is not desired, call `wandb.finish()` before instantiating `WandbLogger`.\n",
      "  rank_zero_warn(\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Dataset path: /Users/takeruito/work/PrfSR/data/training/superfib_r1_dataset.csv\n",
      "üß† Model parameters:\n",
      "  - src_token_num: 80\n",
      "  - tgt_token_num: 15\n",
      "  - token_embed_dim: 16\n",
      "  - max_src_dim: 4\n",
      "  - max_tgt_dim: 859\n",
      "\n",
      "üöÄ Starting chunked training...\n",
      "\n",
      "üì¶ Chunk 1 - Rows: 100,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:13<00:00, 7551.49 examples/s]\n",
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 100000/100000 [00:02<00:00, 39503.40 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'source': [[[2, 114, 0, 0], [225, 114, 0, 0], [2, 558, 0, 0], [114, 336, 0, 0], [1, 2, 0, 0], [1, 447, 0, 0], [336, 1, 0, 0], [2, 225, 0, 0]], [[2, 225, 0, 0], [336, 1, 0, 0], [669, 1, 0, 0], [1, 225, 0, 0], [336, 114, 0, 0], [2, 447, 0, 0], [1, 15, 0, 0], [336, 225, 0, 0], [126, 114, 0, 0], [1, 669, 0, 0], [2, 2, 0, 0], [114, 114, 0, 0], [1, 114, 0, 0]], [[2, 336, 0, 0], [225, 558, 0, 0], [2, 114, 0, 0], [114, 891, 0, 0], [1, 1, 0, 0], [447, 2, 0, 0], [225, 225, 0, 0], [3, 669, 0, 0], [114, 447, 0, 0], [114, 1, 0, 0], [225, 2, 0, 0], [336, 1, 0, 0], [669, 336, 0, 0], [558, 1, 0, 0]], [[336, 1, 0, 0], [1, 225, 0, 0], [1, 891, 0, 0], [225, 669, 0, 0], [1, 1, 0, 0], [26, 336, 0, 0], [225, 1, 0, 0], [115, 1, 0, 0], [2, 2, 0, 0], [1, 114, 0, 0], [2, 669, 0, 0], [3, 558, 0, 0], [37, 114, 0, 0], [1, 669, 0, 0], [37, 447, 0, 0], [558, 225, 0, 0], [780, 558, 0, 0], [114, 447, 0, 0]], [[669, 1, 0, 0], [2, 114, 0, 0], [3, 2, 0, 0], [558, 336, 0, 0], [1, 447, 0, 0], [2, 2, 0, 0], [15, 37, 0, 0], [114, 1, 0, 0], [26, 114, 0, 0], [114, 48, 0, 0], [336, 114, 0, 0], [1, 336, 0, 0], [2, 225, 0, 0], [2, 336, 0, 0], [1, 780, 0, 0], [1, 114, 0, 0]], [[225, 3, 0, 0], [1, 114, 0, 0], [114, 2, 0, 0], [114, 225, 0, 0], [2, 558, 0, 0], [225, 114, 0, 0], [2, 336, 0, 0], [780, 558, 0, 0], [81, 447, 0, 0], [447, 2, 0, 0], [669, 669, 0, 0], [1, 780, 0, 0], [2, 1, 0, 0], [225, 1, 0, 0], [48, 114, 0, 0], [1, 447, 0, 0], [1, 1, 0, 0]], [[114, 2, 0, 0], [669, 114, 0, 0], [558, 70, 0, 0], [336, 558, 0, 0], [225, 891, 0, 0], [225, 1, 0, 0], [2, 225, 0, 0], [336, 2, 0, 0], [1, 780, 0, 0], [1, 225, 0, 0], [1, 1, 0, 0], [37, 780, 0, 0], [2, 2, 0, 0], [1, 114, 0, 0], [1, 447, 0, 0], [37, 225, 0, 0], [114, 114, 0, 0]], [[114, 1, 0, 0], [2, 2, 0, 0], [447, 1, 0, 0], [114, 225, 0, 0], [114, 336, 0, 0], [2, 1, 0, 0], [336, 2, 0, 0], [2, 669, 0, 0], [1, 1, 0, 0], [225, 669, 0, 0], [447, 3, 0, 0], [59, 447, 0, 0], [225, 1, 0, 0], [336, 114, 0, 0], [1, 891, 0, 0], [114, 558, 0, 0]], [[2, 114, 0, 0], [2, 2, 0, 0], [447, 2, 0, 0], [336, 447, 0, 0], [336, 2, 0, 0], [1, 1, 0, 0], [225, 1, 0, 0], [114, 2, 0, 0], [1, 2, 0, 0], [2, 225, 0, 0], [1, 225, 0, 0], [114, 1, 0, 0]], [[447, 225, 0, 0], [336, 37, 0, 0], [558, 2, 0, 0], [1, 558, 0, 0], [558, 1, 0, 0], [114, 1, 0, 0], [1, 1, 0, 0], [1, 114, 0, 0], [26, 1, 0, 0], [2, 114, 0, 0], [114, 2, 0, 0], [114, 336, 0, 0]]], 'target': [[1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 7, 4, 5, 12, 3, 11, 3, 8, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 9, 4, 5, 11, 3, 9, 5, 6, 4, 5, 14, 3, 4, 5, 14, 3, 4, 5, 14, 3, 4, 4, 5, 13, 3, 4, 4, 2], [1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 7, 4, 5, 12, 3, 11, 3, 8, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 9, 4, 5, 11, 3, 9, 5, 6, 4, 5, 14, 3, 4, 5, 14, 3, 4, 5, 14, 3, 4, 4, 5, 11, 3, 6, 5, 6, 4, 4, 2], [1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 7, 4, 5, 12, 3, 11, 3, 8, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 9, 4, 5, 11, 3, 9, 5, 6, 4, 5, 14, 3, 4, 5, 14, 3, 4, 5, 14, 3, 4, 4, 5, 10, 3, 13, 3, 4, 5, 13, 3, 4, 4, 4, 2], [1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 7, 4, 5, 12, 3, 11, 3, 8, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 9, 4, 5, 11, 3, 9, 5, 6, 4, 5, 14, 3, 4, 5, 14, 3, 4, 5, 14, 3, 4, 4, 5, 12, 3, 11, 3, 6, 5, 6, 4, 5, 11, 3, 7, 5, 6, 4, 5, 14, 3, 4, 4, 4, 2], [1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 9, 4, 5, 13, 3, 4, 5, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 8, 5, 8, 4, 5, 11, 3, 8, 5, 6, 4, 5, 14, 3, 4, 5, 14, 3, 4, 4, 4, 2], [1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 9, 4, 5, 13, 3, 4, 5, 12, 3, 11, 3, 8, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 9, 4, 5, 11, 3, 9, 5, 6, 4, 5, 14, 3, 4, 5, 14, 3, 4, 5, 14, 3, 4, 4, 4, 2], [1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 10, 3, 13, 3, 4, 5, 11, 3, 9, 5, 6, 4, 4, 5, 13, 3, 4, 5, 12, 3, 11, 3, 8, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 11, 3, 9, 5, 9, 4, 5, 11, 3, 9, 5, 6, 4, 5, 14, 3, 4, 5, 14, 3, 4, 5, 14, 3, 4, 4, 4, 2], [1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 10, 3, 13, 3, 4, 5, 11, 3, 9, 5, 7, 4, 4, 5, 13, 3, 4, 5, 13, 3, 4, 4, 2], [1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 10, 3, 13, 3, 4, 5, 11, 3, 9, 5, 7, 4, 4, 5, 13, 3, 4, 5, 11, 3, 6, 5, 6, 4, 4, 2], [1, 12, 3, 11, 3, 7, 5, 6, 4, 5, 11, 3, 9, 5, 8, 4, 5, 10, 3, 13, 3, 4, 5, 11, 3, 9, 5, 7, 4, 4, 5, 13, 3, 4, 5, 10, 3, 13, 3, 4, 5, 13, 3, 4, 4, 4, 2]], 'num_points': [8, 13, 14, 18, 16, 17, 17, 16, 12, 12]}\n",
      "Train dataset: 99000 samples\n",
      "Validation dataset: 1000 samples\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/callbacks/model_checkpoint.py:604: UserWarning: Checkpoint directory /Users/takeruito/work/PrfSR/logs/checkpoints exists and is not empty.\n",
      "  rank_zero_warn(f\"Checkpoint directory {dirpath} exists and is not empty.\")\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | src_embedding | Embedding          | 1.3 K \n",
      "1 | fc1           | Linear             | 4.2 K \n",
      "2 | fc2           | Linear             | 4.2 K \n",
      "3 | tgt_embedding | Embedding          | 960   \n",
      "4 | pos_enc       | PositionalEncoding | 0     \n",
      "5 | transformer   | Transformer        | 1.1 M \n",
      "6 | fc_out        | Linear             | 975   \n",
      "7 | loss_fn       | CrossEntropyLoss   | 0     \n",
      "-----------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.434     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "source shape:  torch.Size([26, 7, 4])\n",
      "target shape:  torch.Size([26, 166])\n",
      "\n",
      "source shape:  torch.Size([24, 9, 4])\n",
      "target shape:  torch.Size([24, 162])\n",
      "\n",
      "source shape:  torch.Size([29, 9, 4])\n",
      "target shape:  torch.Size([29, 125])\n",
      "\n",
      "source shape:  torch.Size([25, 8, 4])\n",
      "target shape:  torch.Size([25, 195])\n",
      "\n",
      "source shape:  torch.Size([29, 8, 4])\n",
      "target shape:  torch.Size([29, 177])\n",
      "\n",
      "source shape:  torch.Size([33, 9, 4])\n",
      "target shape:  torch.Size([33, 107])\n",
      "\n",
      "source shape:  torch.Size([27, 10, 4])\n",
      "target shape:  torch.Size([27, 195])\n",
      "\n",
      "source shape:  torch.Size([23, 9, 4])\n",
      "target shape:  torch.Size([23, 170])\n",
      "\n",
      "source shape:  torch.Size([26, 9, 4])\n",
      "target shape:  torch.Size([26, 159])\n",
      "\n",
      "source shape:  torch.Size([26, 9, 4])\n",
      "target shape:  torch.Size([26, 177])\n",
      "\n",
      "source shape:  torch.Size([25, 9, 4])\n",
      "target shape:  torch.Size([25, 160])\n",
      "üî• Training on chunk 1...\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/1 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   5%|‚ñå         | 201/3785 [01:10<20:53,  2.86it/s, loss=1.04, v_num=hzq7, train_loss=1.050]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved. New best score: 1.015\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  11%|‚ñà         | 402/3785 [03:03<25:42,  2.19it/s, loss=0.839, v_num=hzq7, train_loss=0.835, val_loss=1.010]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric val_loss improved by 0.287 >= min_delta = 0.0. New best score: 0.728\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  14%|‚ñà‚ñç        | 547/3785 [04:16<25:16,  2.14it/s, loss=0.663, v_num=hzq7, train_loss=0.672, val_loss=0.728]"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    "    dirpath=root_path / \"logs\" / \"checkpoints\",\n",
    "    filename='model-{epoch:02d}-{val_loss:.2f}'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize WandB logger\n",
    "wandb_logger = WandbLogger(\n",
    "    project=\"prfsr-chunked-training\",\n",
    "    name=f\"chunk_training_{config.batching_strategy}\",\n",
    "    log_model=True,\n",
    "    save_dir= root_path / \"logs\",\n",
    ")\n",
    "\n",
    "\n",
    "# Initialize variables\n",
    "dataset_path = config.data_path\n",
    "total_samples = 0\n",
    "losses = []\n",
    "chunk_metrics = []\n",
    "\n",
    "print(f\"üìÅ Dataset path: {dataset_path}\")\n",
    "print(f\"üß† Model parameters:\")\n",
    "print(f\"  - src_token_num: {metadata.max_src_points}\")\n",
    "print(f\"  - tgt_token_num: {len(metadata.tgt_vocab_list)}\")\n",
    "print(f\"  - token_embed_dim: {config.token_embed_dim}\")\n",
    "print(f\"  - max_src_dim: {metadata.max_point_dim}\")\n",
    "print(f\"  - max_tgt_dim: {metadata.max_tgt_length}\")\n",
    "\n",
    "chunk_reader = pd.read_csv(dataset_path, chunksize=100000)\n",
    "model = LitTransformer(\n",
    "    src_token_num=metadata.max_src_points,\n",
    "    tgt_token_num=len(metadata.tgt_vocab_list),\n",
    "    token_embed_dim=config.token_embed_dim,\n",
    "    max_src_dim=metadata.max_point_dim,\n",
    "    max_tgt_dim=metadata.max_tgt_length,\n",
    "    src_padding_idx=metadata.src_inv_vocab['[PAD]'],\n",
    "    tgt_padding_idx=metadata.tgt_inv_vocab['[PAD]'],\n",
    "    emb_expansion_factor=config.emb_expansion_factor,\n",
    "    t_config=config.transformer,  # Now this will work with dot notation in train.py\n",
    "    learning_rate=eval(config.learning_rate),\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    log_every_n_steps=10,\n",
    "    max_epochs=config.max_epoch,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    logger=wandb_logger,  # Add WandB logger\n",
    "    enable_progress_bar=True,\n",
    "    val_check_interval=200,\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ Starting chunked training...\")\n",
    "\n",
    "for chunk_idx, chunk_df in enumerate(chunk_reader):\n",
    "    print(f\"\\nüì¶ Chunk {chunk_idx+1} - Rows: {len(chunk_df):,}\")\n",
    "    \n",
    "    # Convert DataFrame to Dataset\n",
    "    dataset = Dataset.from_pandas(chunk_df)\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\n",
    "            \"source\": eval(x[\"source\"]),\n",
    "            \"target\": eval(x[\"target\"]),\n",
    "        },\n",
    "        batched=False,\n",
    "        num_proc=1\n",
    "    )\n",
    "    dataset = dataset.map(lambda x: {\"num_points\": len(x[\"source\"])}, batched=False, num_proc=1)\n",
    "    dataset.sort(\"num_points\", reverse=True)  # Sort by num_points in descending order\n",
    "    print(dataset[:10])\n",
    "    \n",
    "    # Create DataModule\n",
    "    datamodule = CSVDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=0,  # config.num_workers,\n",
    "        train_val_split=1 - config.test_ratio,\n",
    "        seed=42,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        batching_strategy=config.batching_strategy,\n",
    "        min_tokens_per_batch=config.min_n_tokens_in_batch,\n",
    "        max_batch_size=config.batch_size,\n",
    "    )\n",
    "     \n",
    "    # show batch size\n",
    "    datamodule.setup()\n",
    "    for i, batch in enumerate(datamodule.train_dataloader()):\n",
    "        print(\"source shape: \", batch[0].shape)\n",
    "        print(\"target shape: \", batch[1].shape)\n",
    "        if i >= 10:  # Only print the first batch\n",
    "            break\n",
    "        print()\n",
    "    \n",
    "    \n",
    "    # Training\n",
    "    print(f\"üî• Training on chunk {chunk_idx+1}...\")\n",
    "    trainer.fit(model, datamodule)\n",
    "    \n",
    "    # Log results\n",
    "    if hasattr(trainer, 'logged_metrics') and trainer.logged_metrics:\n",
    "        val_loss = trainer.logged_metrics.get('val_loss', 0)\n",
    "        train_loss = trainer.logged_metrics.get('train_loss', 0)\n",
    "    elif hasattr(trainer, 'callback_metrics') and trainer.callback_metrics:\n",
    "        val_loss = trainer.callback_metrics.get('val_loss', 0)\n",
    "        train_loss = trainer.callback_metrics.get('train_loss', 0)\n",
    "    else:\n",
    "        val_loss = 0.0\n",
    "        train_loss = 0.0\n",
    "    \n",
    "    total_samples += len(chunk_df)\n",
    "    losses.append(float(val_loss))\n",
    "    \n",
    "    # Collect chunk metrics\n",
    "    chunk_metrics.append({\n",
    "        'chunk_idx': chunk_idx + 1,\n",
    "        'chunk_size': len(chunk_df),\n",
    "        'val_loss': float(val_loss),\n",
    "        'train_loss': float(train_loss),\n",
    "        'total_samples': total_samples\n",
    "    })\n",
    "    \n",
    "    # Log chunk metrics to WandB\n",
    "    wandb.log({\n",
    "        'chunk_idx': chunk_idx + 1,\n",
    "        'chunk_size': len(chunk_df),\n",
    "        'chunk_val_loss': float(val_loss),\n",
    "        'chunk_train_loss': float(train_loss),\n",
    "        'total_samples_processed': total_samples,\n",
    "        'cumulative_chunks': chunk_idx + 1\n",
    "    })\n",
    "    \n",
    "    print(f\"‚úÖ Chunk {chunk_idx+1} completed:\")\n",
    "    print(f\"  - Train Loss: {train_loss:.4f}\")\n",
    "    print(f\"  - Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  - Total samples processed: {total_samples:,}\")\n",
    "    \n",
    "print(f\"\\nüéâ Training completed!\")\n",
    "print(f\"üìä Total samples processed: {total_samples:,}\")\n",
    "print(f\"üìà Loss progression: {[f'{loss:.4f}' for loss in losses]}\")\n",
    "\n",
    "# Log final summary to WandB\n",
    "wandb.log({\n",
    "    'final_total_samples': total_samples,\n",
    "    'final_avg_val_loss': np.mean(losses),\n",
    "    'final_best_val_loss': min(losses),\n",
    "    'total_chunks_processed': len(losses)\n",
    "})\n",
    "\n",
    "# Create summary table for WandB\n",
    "import pandas as pd\n",
    "chunk_summary_df = pd.DataFrame(chunk_metrics)\n",
    "wandb.log({\"chunk_summary\": wandb.Table(dataframe=chunk_summary_df)})\n",
    "\n",
    "print(f\"\\nüìä WandB logging completed!\")\n",
    "print(f\"üîó View your run at: {wandb.run.url}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
