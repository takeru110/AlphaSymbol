#!/usr/bin/env python3
"""
Dataset File Creator

raw.csvãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ã€ãƒ¢ãƒ‡ãƒ«å­¦ç¿’ç”¨ã®æ•°å€¤åŒ–ã•ã‚ŒãŸdataset.csvã‚’ä½œæˆã™ã‚‹ã€‚

æ©Ÿèƒ½:
- raw.csvã®inputs, outputs, exprã‚«ãƒ©ãƒ ã‹ã‚‰source, targetã‚«ãƒ©ãƒ ã‚’ç”Ÿæˆ
- metadata.yamlã®èªå½™ãƒªã‚¹ãƒˆã‚’ä½¿ç”¨ã—ã¦ãƒˆãƒ¼ã‚¯ãƒ³ã‚’æ•°å€¤åŒ–
- sourceã¯2æ¬¡å…ƒé…åˆ—ã‚’ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã—ã¦æ•°å€¤åŒ–
- targetã¯exprã‚’BOS/EOSãƒˆãƒ¼ã‚¯ãƒ³ä»˜ãã§æ•°å€¤åŒ–
"""

import argparse
import ast
import logging
import multiprocessing as mp
import os
import re
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Any, Dict, List, Tuple

import pandas as pd
import yaml


def setup_logging(verbose: bool) -> None:
    """ãƒ­ã‚°è¨­å®šã‚’åˆæœŸåŒ–"""
    level = logging.DEBUG if verbose else logging.INFO
    logging.basicConfig(
        level=level, format="%(asctime)s - %(levelname)s - %(message)s"
    )


def create_default_metadata_path(input_path: str) -> str:
    """å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
    if input_path.endswith(".csv"):
        return input_path[:-4] + "_metadata.yaml"
    return input_path + "_metadata.yaml"


def create_default_output_path(input_path: str) -> str:
    """å…¥åŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‹ã‚‰ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆã®å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ã‚’ç”Ÿæˆ"""
    if input_path.endswith(".csv"):
        return input_path[:-4] + "_dataset.csv"
    return input_path + "_dataset.csv"


def print_arguments(
    input_path: str, metadata_path: str, output_path: str
) -> None:
    """ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®å€¤ã‚’è¡¨ç¤º"""
    print("ğŸ“‹ Command line arguments:")
    print("=" * 50)
    print(f"ğŸ“ Input (-i):     {input_path}")
    print(f"ğŸ“‹ Metadata (-m):  {metadata_path}")
    print(f"ğŸ’¾ Output (-o):    {output_path}")
    print("=" * 50)


def load_metadata(metadata_path: str) -> Dict[str, Any]:
    """ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿YAMLãƒ•ã‚¡ã‚¤ãƒ«ã‚’èª­ã¿è¾¼ã¿"""
    with open(metadata_path, "r", encoding="utf-8") as f:
        metadata = yaml.safe_load(f)

    logging.info(f"Loaded metadata from: {metadata_path}")
    logging.debug(f"Metadata keys: {list(metadata.keys())}")

    return metadata


def process_input_to_source(
    input_str: str,
    src_vocab_list: List[str],
    max_point_dim: int,
    pad_index: int,
) -> str:
    """inputsã‚«ãƒ©ãƒ ã®å€¤ã‚’sourceã‚«ãƒ©ãƒ ã®å€¤ã«å¤‰æ›"""
    try:
        # æ–‡å­—åˆ—ã‚’Pythonã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›
        input_data = ast.literal_eval(input_str)

        # å„ç‚¹ã‚’èªå½™ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
        indexed_points = []
        for point in input_data:
            indexed_point = []
            for value in point:
                str_value = str(value)
                if str_value in src_vocab_list:
                    indexed_point.append(src_vocab_list.index(str_value))
                else:
                    logging.warning(
                        f"Unknown token '{str_value}' not in src_vocab_list"
                    )
                    indexed_point.append(
                        pad_index
                    )  # æœªçŸ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯PADã§åŸ‹ã‚ã‚‹

            # max_point_dimã¾ã§ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            while len(indexed_point) < max_point_dim:
                indexed_point.append(pad_index)

            # max_point_dimã‚’è¶…ãˆã‚‹å ´åˆã¯åˆ‡ã‚Šæ¨ã¦
            indexed_point = indexed_point[:max_point_dim]

            indexed_points.append(indexed_point)

        # çµæœã‚’æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™
        return str(indexed_points)

    except (ValueError, SyntaxError) as e:
        logging.error(f"Error parsing input: {input_str}, error: {e}")
        return str(
            [[pad_index] * max_point_dim]
        )  # ã‚¨ãƒ©ãƒ¼æ™‚ã¯PADã§åŸ‹ã‚ãŸå˜ä¸€ç‚¹ã‚’è¿”ã™


def process_expr_to_target(expr_str: str, tgt_vocab_list: List[str]) -> str:
    """exprã‚«ãƒ©ãƒ ã®å€¤ã‚’targetã‚«ãƒ©ãƒ ã®å€¤ã«å¤‰æ›"""
    try:
        # ç©ºç™½ã‚’stripã™ã‚‹
        cleaned_expr = expr_str.strip()

        # å„æ–‡å­—ã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆé–¢æ•°åã€æ•°å­—ã€æ‹¬å¼§ã€ã‚«ãƒ³ãƒã‚’æŠ½å‡ºï¼‰
        tokens = re.findall(r"[ZSPCRPRF]+|\d+|[\(\),]", cleaned_expr)

        # ãƒˆãƒ¼ã‚¯ãƒ³ã‚’èªå½™ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã«å¤‰æ›
        indexed_tokens = []

        # [BOS]ã‚’å…ˆé ­ã«è¿½åŠ 
        bos_index = tgt_vocab_list.index("[BOS]")
        indexed_tokens.append(bos_index)

        # å„ãƒˆãƒ¼ã‚¯ãƒ³ã‚’å¤‰æ›
        for token in tokens:
            if token in tgt_vocab_list:
                indexed_tokens.append(tgt_vocab_list.index(token))
            else:
                logging.warning(
                    f"Unknown token '{token}' not in tgt_vocab_list"
                )
                # æœªçŸ¥ã®ãƒˆãƒ¼ã‚¯ãƒ³ã¯[PAD]ã§ä»£æ›¿ï¼ˆã‚¨ãƒ©ãƒ¼å‡¦ç†ï¼‰
                pad_index = tgt_vocab_list.index("[PAD]")
                indexed_tokens.append(pad_index)

        # [EOS]ã‚’æœ«å°¾ã«è¿½åŠ 
        eos_index = tgt_vocab_list.index("[EOS]")
        indexed_tokens.append(eos_index)

        # çµæœã‚’æ–‡å­—åˆ—ã¨ã—ã¦è¿”ã™ï¼ˆãƒ‘ãƒ‡ã‚£ãƒ³ã‚°ã¯è¡Œã‚ãªã„ï¼‰
        return str(indexed_tokens)

    except (ValueError, KeyError) as e:
        logging.error(f"Error processing expr: {expr_str}, error: {e}")
        # ã‚¨ãƒ©ãƒ¼æ™‚ã¯[BOS][PAD][EOS]ã®ã¿è¿”ã™
        try:
            bos_index = tgt_vocab_list.index("[BOS]")
            eos_index = tgt_vocab_list.index("[EOS]")
            pad_index = tgt_vocab_list.index("[PAD]")
            return str([bos_index, pad_index, eos_index])
        except ValueError:
            return str([0, 0, 0])  # æœ€å¾Œã®æ‰‹æ®µ


def process_csv_chunk(
    chunk_df: pd.DataFrame, metadata: Dict[str, Any]
) -> pd.DataFrame:
    """CSVã®ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ã—ã¦sourceã¨targetã‚«ãƒ©ãƒ ã‚’ä½œæˆ"""
    src_vocab_list = metadata["src_vocab_list"]
    tgt_vocab_list = metadata["tgt_vocab_list"]
    max_point_dim = metadata["max_point_dim"]
    pad_index = src_vocab_list.index("[PAD]")

    # inputsã‚«ãƒ©ãƒ ãŒå­˜åœ¨ã™ã‚‹ã‹ãƒã‚§ãƒƒã‚¯ï¼ˆinput vs inputsï¼‰
    input_col = None
    if "inputs" in chunk_df.columns:
        input_col = "inputs"
    elif "input" in chunk_df.columns:
        input_col = "input"
    else:
        raise ValueError("Neither 'inputs' nor 'input' column found in CSV")

    # exprã‚«ãƒ©ãƒ ã®å­˜åœ¨ç¢ºèª
    if "expr" not in chunk_df.columns:
        raise ValueError("'expr' column not found in CSV")

    logging.debug(f"Using input column: {input_col}")

    # sourceã¨targetã‚«ãƒ©ãƒ ã‚’ä½œæˆ
    source_data = []
    target_data = []

    for _, row in chunk_df.iterrows():
        # sourceã‚«ãƒ©ãƒ ã®å‡¦ç†
        input_value = row[input_col]
        if pd.notna(input_value):
            source_value = process_input_to_source(
                str(input_value), src_vocab_list, max_point_dim, pad_index
            )
            source_data.append(source_value)
        else:
            # NaNã®å ´åˆã¯PADã§åŸ‹ã‚ãŸå˜ä¸€ç‚¹
            pad_point = str([[pad_index] * max_point_dim])
            source_data.append(pad_point)

        # targetã‚«ãƒ©ãƒ ã®å‡¦ç†
        expr_value = row["expr"]
        if pd.notna(expr_value):
            target_value = process_expr_to_target(
                str(expr_value), tgt_vocab_list
            )
            target_data.append(target_value)
        else:
            # NaNã®å ´åˆã¯[BOS][PAD][EOS]
            try:
                bos_index = tgt_vocab_list.index("[BOS]")
                eos_index = tgt_vocab_list.index("[EOS]")
                pad_index_tgt = tgt_vocab_list.index("[PAD]")
                fallback_target = str([bos_index, pad_index_tgt, eos_index])
                target_data.append(fallback_target)
            except ValueError:
                target_data.append(str([0, 0, 0]))

    # çµæœã®DataFrameã‚’ä½œæˆ
    result_df = pd.DataFrame({"source": source_data, "target": target_data})

    return result_df


def process_chunk_parallel(
    chunk_data: Tuple[int, pd.DataFrame, Dict[str, Any]],
) -> pd.DataFrame:
    """ä¸¦åˆ—å‡¦ç†ç”¨ã®ãƒãƒ£ãƒ³ã‚¯å‡¦ç†é–¢æ•°"""
    chunk_id, chunk_df, metadata = chunk_data
    return process_csv_chunk(chunk_df, metadata)


def process_csv_parallel(
    input_path: str,
    metadata: Dict[str, Any],
    n_workers: int = None,
    chunk_size: int = 1000,
) -> pd.DataFrame:
    """ä¸¦åˆ—å‡¦ç†ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
    if n_workers is None:
        n_workers = min(mp.cpu_count(), 8)  # æœ€å¤§8ãƒ—ãƒ­ã‚»ã‚¹

    logging.info(f"Starting parallel processing with {n_workers} workers")

    # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦DataFrameã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    chunks = []
    chunk_id = 0

    print("ğŸ“š Loading and splitting CSV into chunks...")
    chunk_reader = pd.read_csv(input_path, chunksize=chunk_size)

    for chunk_df in chunk_reader:
        chunks.append((chunk_id, chunk_df, metadata))
        chunk_id += 1

    logging.info(f"Created {len(chunks)} chunks for processing")

    # ä¸¦åˆ—å‡¦ç†ã§å„ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
    output_chunks = []

    print(f"âš¡ Processing {len(chunks)} chunks with {n_workers} workers...")

    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        # å…¨ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚µãƒ–ãƒŸãƒƒãƒˆ
        futures = {
            executor.submit(process_chunk_parallel, chunk): chunk[0]
            for chunk in chunks
        }

        # é€²æ—ãƒãƒ¼ä»˜ãã§çµæœã‚’å–å¾—
        completed = 0
        total_chunks = len(chunks)

        for future in as_completed(futures):
            chunk_id = futures[future]
            try:
                processed_chunk = future.result()
                output_chunks.append(processed_chunk)

                completed += 1
                if completed % 10 == 0 or completed == total_chunks:  # é€²æ—è¡¨ç¤º
                    print(f"  Processed {completed}/{total_chunks} chunks")

            except Exception as e:
                logging.error(f"Error processing chunk {chunk_id}: {e}")

    # çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
    if output_chunks:
        return pd.concat(output_chunks, ignore_index=True)
    else:
        return pd.DataFrame()


def process_csv_sequential(
    input_path: str, metadata: Dict[str, Any], chunk_size: int = 1000
) -> pd.DataFrame:
    """é †æ¬¡å‡¦ç†ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’å‡¦ç†"""
    output_chunks = []
    total_rows = 0
    chunk_count = 0

    for chunk_df in pd.read_csv(input_path, chunksize=chunk_size):
        chunk_count += 1
        total_rows += len(chunk_df)

        # ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
        processed_chunk = process_csv_chunk(chunk_df, metadata)
        output_chunks.append(processed_chunk)

        if chunk_count % 10 == 0:  # 10ãƒãƒ£ãƒ³ã‚¯ã”ã¨ã«é€²æ—è¡¨ç¤º
            print(f"  Processed {chunk_count} chunks ({total_rows:,} rows)")

    # çµæœã‚’ã¾ã¨ã‚ã¦è¿”ã™
    if output_chunks:
        return pd.concat(output_chunks, ignore_index=True)
    else:
        return pd.DataFrame()


def main() -> int:
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Create dataset.csv from raw.csv and metadata.yaml",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Examples:
  # åŸºæœ¬çš„ãªä½¿ç”¨æ³•
  python dataset_file.py -i data/raw.csv
  
  # å…¨ã‚ªãƒ—ã‚·ãƒ§ãƒ³æŒ‡å®š
  python dataset_file.py -i data/raw.csv -m data/metadata.yaml -o data/dataset.csv
  
  # è©³ç´°ãƒ­ã‚°ä»˜ã
  python dataset_file.py -i data/raw.csv -v
  
  # ä¸¦åˆ—å‡¦ç†ï¼ˆ4ãƒ¯ãƒ¼ã‚«ãƒ¼ã€ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º500ï¼‰
  python dataset_file.py -i data/raw.csv -p -w 4 -c 500
        """,
    )

    # å¿…é ˆå¼•æ•°
    parser.add_argument(
        "-i",
        "--input",
        required=True,
        help="Input raw CSV file path (required)",
    )

    # ã‚ªãƒ—ã‚·ãƒ§ãƒ³å¼•æ•°
    parser.add_argument(
        "-m",
        "--metadata",
        help="Metadata YAML file path (default: {input}_metadata.yaml)",
    )

    parser.add_argument(
        "-o",
        "--output",
        help="Output dataset CSV file path (default: {input}_dataset.csv)",
    )

    parser.add_argument(
        "-v", "--verbose", action="store_true", help="Enable verbose logging"
    )

    parser.add_argument(
        "-p",
        "--parallel",
        action="store_true",
        help="Enable parallel processing",
    )

    parser.add_argument(
        "-w",
        "--workers",
        type=int,
        default=None,
        help="Number of worker processes (default: CPU count, max 8)",
    )

    parser.add_argument(
        "-c",
        "--chunk-size",
        type=int,
        default=1000,
        help="Chunk size for processing (default: 1000)",
    )

    args = parser.parse_args()

    # ãƒ­ã‚°è¨­å®š
    setup_logging(args.verbose)

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
    input_path = args.input
    metadata_path = args.metadata or create_default_metadata_path(input_path)
    output_path = args.output or create_default_output_path(input_path)

    # å¼•æ•°ã®å€¤ã‚’è¡¨ç¤º
    print_arguments(input_path, metadata_path, output_path)

    try:
        # ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿èª­ã¿è¾¼ã¿
        print("\nğŸ“‹ Loading metadata...")
        metadata = load_metadata(metadata_path)

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®äº‹å‰ç¢ºèª
        file_size_mb = os.path.getsize(input_path) / (1024 * 1024)
        total_rows = sum(1 for _ in open(input_path)) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã

        print(f"ğŸ“Š File info: {file_size_mb:.2f} MB, {total_rows:,} rows")

        # ä¸¦åˆ—å‡¦ç†ã‹é †æ¬¡å‡¦ç†ã‹ã‚’é¸æŠ
        if args.parallel:
            print("\nâš¡ Starting parallel processing...")
            print(
                f"ğŸ”§ Workers: {args.workers or 'auto'}, Chunk size: {args.chunk_size}"
            )

            final_df = process_csv_parallel(
                input_path,
                metadata,
                n_workers=args.workers,
                chunk_size=args.chunk_size,
            )
        else:
            print("\nğŸš€ Starting sequential processing...")
            final_df = process_csv_sequential(
                input_path, metadata, chunk_size=args.chunk_size
            )

        # çµæœã‚’ä¿å­˜
        print("ğŸ’¾ Saving results...")
        final_df.to_csv(output_path, index=False, encoding="utf-8")

        print("\nâœ… Processing completed!")
        print(f"ğŸ“Š Total processed: {len(final_df):,} samples")
        print(f"ğŸ’¾ Output saved to: {output_path}")

        return 0

    except Exception as e:
        logging.error(f"âŒ Error: {e}")
        if args.verbose:
            import traceback

            traceback.print_exc()
        return 1


if __name__ == "__main__":
    main()
