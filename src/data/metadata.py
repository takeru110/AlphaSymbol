#!/usr/bin/env python3
"""
ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ãƒ„ãƒ¼ãƒ«

PRFDatasetã®åˆæœŸåŒ–å‡¦ç†ã‚’å‚è€ƒã«ã—ã¦ã€ä»¥ä¸‹ã®å€¤ã‚’è¨ˆç®—ã—YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã™ã‚‹ï¼š
- src_vocab_list: ã‚½ãƒ¼ã‚¹èªå½™ãƒªã‚¹ãƒˆ
- tgt_vocab_list: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèªå½™ãƒªã‚¹ãƒˆ
- max_tgt_length: æœ€å¤§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé•·
- max_src_points: æœ€å¤§ã‚½ãƒ¼ã‚¹ç‚¹æ•°
"""

import argparse
import logging
import multiprocessing as mp
import os
import re
from concurrent.futures import ProcessPoolExecutor, as_completed
from typing import Any, Dict, List, Set, Tuple

import pandas as pd
import yaml
from tqdm import tqdm


def encode_expr(expr_str: str) -> List[str]:
    """exprã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å‡¦ç†ã‚’æ¨¡æ“¬ï¼‰"""
    # é–¢æ•°åï¼ˆZ,S,P,C,Rï¼‰ã€æ•°å­—ã€æ‹¬å¼§ã€ã‚«ãƒ³ãƒã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŠ½å‡º
    tokens = re.findall(r"[ZSPCRPRF]+|\d+|[\(\),\s]", expr_str)
    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
    tokens = [token for token in tokens if token.strip()]
    # BOS, EOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
    return ["[BOS]"] + tokens + ["[EOS]"]


def process_chunk(chunk_data: Tuple[int, pd.DataFrame]) -> Tuple[Set[str], Set[str], int, int, int]:
    """
    ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†ã—ã¦ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºã™ã‚‹é–¢æ•°ï¼ˆä¸¦åˆ—å‡¦ç†ç”¨ï¼‰
    
    Args:
        chunk_data: (chunk_id, DataFrame) ã®ã‚¿ãƒ—ãƒ«
        
    Returns:
        Tuple[src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points, skipped_count]
    """
    chunk_id, chunk_df = chunk_data
    
    src_vocab = set()
    tgt_vocab = set()
    max_tgt_length = 0
    max_src_points = 0
    skipped_count = 0
    
    for _, row in chunk_df.iterrows():
        # 1. srcèªå½™ã®æŠ½å‡ºï¼ˆinput/outputï¼‰
        if "input" in row and pd.notna(row["input"]):
            input_str = str(row["input"])
            # æ•°å­—ã€æ‹¬å¼§ã€ã‚«ãƒ³ãƒã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŠ½å‡º
            tokens = re.findall(r"\d+|[\[\],\(\)\s]", input_str)
            src_vocab.update(tokens)

            # 3. max_src_pointsã®è¨ˆç®—
            try:
                input_data = (
                    eval(input_str)
                    if isinstance(input_str, str)
                    else input_str
                )
                points = len(input_data)
                max_src_points = max(max_src_points, points)
            except (ValueError, SyntaxError, TypeError):
                skipped_count += 1

        if "output" in row and pd.notna(row["output"]):
            output_str = str(row["output"])
            tokens = re.findall(r"\d+|[\[\],\(\)\s]", output_str)
            src_vocab.update(tokens)

        # 2. tgtèªå½™ã®æŠ½å‡ºã¨max_tgt_lengthã®è¨ˆç®—ï¼ˆexprï¼‰
        if "expr" in row and pd.notna(row["expr"]):
            expr_str = str(row["expr"])

            # tgtèªå½™ã®æŠ½å‡º
            tokens = re.findall(
                r"[ZSPCRPRF]+|\d+|[\(\),\s]", expr_str
            )
            tgt_vocab.update(tokens)

            # max_tgt_lengthã®è¨ˆç®—
            encoded_tokens = encode_expr(expr_str)
            max_tgt_length = max(
                max_tgt_length, len(encoded_tokens)
            )
    
    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
    src_vocab.discard("")
    tgt_vocab.discard("")
    
    return src_vocab, tgt_vocab, max_tgt_length, max_src_points, skipped_count


def calculate_metadata_parallel(
    csv_path: str,
    n_workers: int = None,
    chunk_size: int = 1000
) -> Tuple[Set[str], Set[str], int, int]:
    """
    ä¸¦åˆ—å‡¦ç†ã§CSVãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—
    
    Args:
        csv_path: CSVãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒ‘ã‚¹
        n_workers: ãƒ¯ãƒ¼ã‚«ãƒ¼ãƒ—ãƒ­ã‚»ã‚¹æ•°ï¼ˆNoneã®å ´åˆã¯CPUæ•°ï¼‰
        chunk_size: ãƒãƒ£ãƒ³ã‚¯ã‚µã‚¤ã‚º
        
    Returns:
        Tuple[src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points]
    """
    if n_workers is None:
        n_workers = min(mp.cpu_count(), 8)  # æœ€å¤§8ãƒ—ãƒ­ã‚»ã‚¹
    
    logging.info(f"Starting parallel processing with {n_workers} workers")
    
    # ãƒ•ã‚¡ã‚¤ãƒ«ã®ç·è¡Œæ•°ã‚’å–å¾—
    total_rows = sum(1 for _ in open(csv_path)) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã
    logging.info(f"Total rows to process: {total_rows}")
    
    # ãƒãƒ£ãƒ³ã‚¯ã«åˆ†å‰²ã—ã¦DataFrameã®ãƒªã‚¹ãƒˆã‚’ä½œæˆ
    chunks = []
    chunk_id = 0
    
    print("ğŸ“š Loading and splitting CSV into chunks...")
    chunk_reader = pd.read_csv(csv_path, chunksize=chunk_size)
    
    for chunk_df in tqdm(chunk_reader, desc="Loading chunks"):
        chunks.append((chunk_id, chunk_df))
        chunk_id += 1
    
    logging.info(f"Created {len(chunks)} chunks for processing")
    
    # ä¸¦åˆ—å‡¦ç†ã§å„ãƒãƒ£ãƒ³ã‚¯ã‚’å‡¦ç†
    src_vocab_global = set()
    tgt_vocab_global = set()
    max_tgt_length_global = 0
    max_src_points_global = 0
    total_skipped = 0
    
    print(f"âš¡ Processing {len(chunks)} chunks with {n_workers} workers...")
    
    with ProcessPoolExecutor(max_workers=n_workers) as executor:
        # å…¨ã¦ã®ãƒãƒ£ãƒ³ã‚¯ã‚’ã‚µãƒ–ãƒŸãƒƒãƒˆ
        futures = {executor.submit(process_chunk, chunk): chunk[0] 
                  for chunk in chunks}
        
        # é€²æ—ãƒãƒ¼ä»˜ãã§çµæœã‚’å–å¾—
        with tqdm(total=len(chunks), desc="Processing chunks", unit="chunk") as pbar:
            for future in as_completed(futures):
                chunk_id = futures[future]
                try:
                    src_vocab, tgt_vocab, max_tgt_length, max_src_points, skipped_count = future.result()
                    
                    # çµæœã‚’ãƒãƒ¼ã‚¸
                    src_vocab_global.update(src_vocab)
                    tgt_vocab_global.update(tgt_vocab)
                    max_tgt_length_global = max(max_tgt_length_global, max_tgt_length)
                    max_src_points_global = max(max_src_points_global, max_src_points)
                    total_skipped += skipped_count
                    
                    pbar.set_postfix(
                        src_vocab=len(src_vocab_global),
                        tgt_vocab=len(tgt_vocab_global),
                        max_tgt_len=max_tgt_length_global,
                        max_src_pts=max_src_points_global,
                        skipped=total_skipped,
                        refresh=False
                    )
                    
                except Exception as e:
                    logging.error(f"Error processing chunk {chunk_id}: {e}")
                    
                pbar.update(1)
    
    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
    if max_tgt_length_global == 0:
        max_tgt_length_global = 128
    if max_src_points_global == 0:
        max_src_points_global = 20

    if total_skipped > 0:
        logging.warning(f"Skipped {total_skipped} samples during processing")

    logging.info("Parallel processing completed:")
    logging.info(f"  - Src vocabulary size: {len(src_vocab_global)}")
    logging.info(f"  - Tgt vocabulary size: {len(tgt_vocab_global)}")
    logging.info(f"  - Max target length: {max_tgt_length_global}")
    logging.info(f"  - Max source points: {max_src_points_global}")

    return src_vocab_global, tgt_vocab_global, max_tgt_length_global, max_src_points_global


def calculate_all_metadata(
    csv_path: str,
) -> Tuple[Set[str], Set[str], int, int]:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’1å›ã ã‘èµ°æŸ»ã—ã¦å…¨ã¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—ï¼ˆã‚·ãƒ¼ã‚±ãƒ³ã‚·ãƒ£ãƒ«ç‰ˆï¼‰

    Returns:
        Tuple[src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points]
    """
    src_vocab = set()
    tgt_vocab = set()
    max_tgt_length = 0
    max_src_points = 0
    skipped_count = 0

    try:
        chunk_size = 1000

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ç·è¡Œæ•°ã‚’å–å¾—
        total_rows = sum(1 for _ in open(csv_path)) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã
        logging.info(f"Total rows to process: {total_rows}")

        with tqdm(
            total=total_rows,
            desc="ğŸ” Processing CSV data",
            unit="rows",
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
        ) as pbar:
            for chunk in pd.read_csv(csv_path, chunksize=chunk_size):
                for _, row in chunk.iterrows():
                    # 1. srcèªå½™ã®æŠ½å‡ºï¼ˆinput/outputï¼‰
                    if "input" in row and pd.notna(row["input"]):
                        input_str = str(row["input"])
                        # æ•°å­—ã€æ‹¬å¼§ã€ã‚«ãƒ³ãƒã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŠ½å‡º
                        tokens = re.findall(r"\d+|[\[\],\(\)\s]", input_str)
                        src_vocab.update(tokens)

                        # 3. max_src_pointsã®è¨ˆç®—
                        try:
                            input_data = (
                                eval(input_str)
                                if isinstance(input_str, str)
                                else input_str
                            )
                            points = len(input_data)
                            max_src_points = max(max_src_points, points)
                        except (ValueError, SyntaxError, TypeError) as e:
                            skipped_count += 1
                            logging.debug(
                                f"Skipping input data due to error: {e}"
                            )

                    if "output" in row and pd.notna(row["output"]):
                        output_str = str(row["output"])
                        tokens = re.findall(r"\d+|[\[\],\(\)\s]", output_str)
                        src_vocab.update(tokens)

                    # 2. tgtèªå½™ã®æŠ½å‡ºã¨max_tgt_lengthã®è¨ˆç®—ï¼ˆexprï¼‰
                    if "expr" in row and pd.notna(row["expr"]):
                        expr_str = str(row["expr"])

                        # tgtèªå½™ã®æŠ½å‡º
                        tokens = re.findall(
                            r"[ZSPCRPRF]+|\d+|[\(\),\s]", expr_str
                        )
                        tgt_vocab.update(tokens)

                        # max_tgt_lengthã®è¨ˆç®—
                        encoded_tokens = encode_expr(expr_str)
                        max_tgt_length = max(
                            max_tgt_length, len(encoded_tokens)
                        )

                    pbar.update(1)

                    # çµ±è¨ˆæƒ…å ±ã‚’100è¡Œã”ã¨ã«æ›´æ–°
                    if pbar.n % 100 == 0:
                        pbar.set_postfix(
                            src_vocab=len(src_vocab),
                            tgt_vocab=len(tgt_vocab),
                            max_tgt_len=max_tgt_length,
                            max_src_pts=max_src_points,
                            skipped=skipped_count,
                            refresh=False,
                        )

    except Exception as e:
        logging.warning(f"Chunk reading failed: {e}. Trying normal reading...")
        # ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ãŒå¤±æ•—ã—ãŸå ´åˆã¯é€šå¸¸ã®èª­ã¿è¾¼ã¿
        df = pd.read_csv(csv_path)

        logging.info(f"Fallback: Processing {len(df)} rows in single batch")

        for _, row in tqdm(
            df.iterrows(),
            total=len(df),
            desc="ğŸ” Processing CSV data (fallback)",
            unit="rows",
        ):
            # åŒã˜å‡¦ç†ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã«å®Ÿè¡Œ
            if "input" in row and pd.notna(row["input"]):
                input_str = str(row["input"])
                tokens = re.findall(r"\d+|[\[\],\(\)\s]", input_str)
                src_vocab.update(tokens)

                try:
                    input_data = (
                        eval(input_str)
                        if isinstance(input_str, str)
                        else input_str
                    )
                    points = len(input_data)
                    max_src_points = max(max_src_points, points)
                except (ValueError, SyntaxError, TypeError) as e:
                    skipped_count += 1
                    logging.debug(f"Skipping input data due to error: {e}")

            if "output" in row and pd.notna(row["output"]):
                output_str = str(row["output"])
                tokens = re.findall(r"\d+|[\[\],\(\)\s]", output_str)
                src_vocab.update(tokens)

            if "expr" in row and pd.notna(row["expr"]):
                expr_str = str(row["expr"])
                tokens = re.findall(r"[ZSPCRPRF]+|\d+|[\(\),\s]", expr_str)
                tgt_vocab.update(tokens)

                encoded_tokens = encode_expr(expr_str)
                max_tgt_length = max(max_tgt_length, len(encoded_tokens))

    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
    src_vocab.discard("")
    tgt_vocab.discard("")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
    if max_tgt_length == 0:
        max_tgt_length = 128
    if max_src_points == 0:
        max_src_points = 20

    if skipped_count > 0:
        logging.warning(f"Skipped {skipped_count} samples during processing")

    logging.info("Processing completed:")
    logging.info(f"  - Src vocabulary size: {len(src_vocab)}")
    logging.info(f"  - Tgt vocabulary size: {len(tgt_vocab)}")
    logging.info(f"  - Max target length: {max_tgt_length}")
    logging.info(f"  - Max source points: {max_src_points}")

    return src_vocab, tgt_vocab, max_tgt_length, max_src_points


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Calculate metadata from CSV file and output to YAML"
    )
    parser.add_argument(
        "-i", "--input", required=True, help="Input CSV file path"
    )
    parser.add_argument(
        "-o", "--output", required=True, help="Output YAML file path"
    )
    parser.add_argument(
        "-v", "--verbose", action="store_true", help="Enable verbose logging"
    )
    parser.add_argument(
        "-p", "--parallel", action="store_true", 
        help="Enable parallel processing"
    )
    parser.add_argument(
        "-w", "--workers", type=int, default=None,
        help="Number of worker processes (default: CPU count, max 8)"
    )
    parser.add_argument(
        "-c", "--chunk-size", type=int, default=1000,
        help="Chunk size for processing (default: 1000)"
    )

    args = parser.parse_args()

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    try:
        print(f"ğŸ” Processing CSV file: {args.input}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®äº‹å‰ç¢ºèª
        file_size_mb = os.path.getsize(args.input) / (1024 * 1024)
        total_rows = sum(1 for _ in open(args.input)) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã

        print(f"ğŸ“Š File info: {file_size_mb:.2f} MB, {total_rows:,} rows")

        # ä¸¦åˆ—å‡¦ç†ã‹é †æ¬¡å‡¦ç†ã‹ã‚’é¸æŠ
        if args.parallel:
            print(f"\nâš¡ Starting parallel metadata calculation...")
            print(f"ğŸ”§ Workers: {args.workers or 'auto'}, Chunk size: {args.chunk_size}")
            
            src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points = (
                calculate_metadata_parallel(
                    args.input, 
                    n_workers=args.workers,
                    chunk_size=args.chunk_size
                )
            )
        else:
            print("\nğŸš€ Starting single-pass metadata calculation...")
            
            src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points = (
                calculate_all_metadata(args.input)
            )

        print("\nğŸ”„ Converting vocabularies to sorted lists...")
        # ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
        src_vocab_list = sorted(list(src_vocab_set))
        tgt_vocab_list = sorted(list(tgt_vocab_set))

        # YAMLã«å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆ2ã¤ã®listãŒæœ€å¾Œã«ãªã‚‹ä¸¦ã³ï¼‰
        metadata = {
            "max_tgt_length": max_tgt_length,
            "max_src_points": max_src_points,
            "src_vocab_list": src_vocab_list,
            "tgt_vocab_list": tgt_vocab_list,
        }

        # YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
        print(f"ğŸ’¾ Writing metadata to: {args.output}")
        with open(args.output, "w", encoding="utf-8") as f:
            yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)

        # çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        print("\nâœ… Metadata calculation completed!")
        print("=" * 50)
        print(f"ğŸ“ Max target length:     {max_tgt_length:,}")
        print(f"ğŸ“Š Max source points:     {max_src_points:,}")
        print(f"ğŸ“š Source vocabulary size: {len(src_vocab_list):,}")
        print(f"ğŸ¯ Target vocabulary size: {len(tgt_vocab_list):,}")
        print("=" * 50)
        print(f"ğŸ’¾ Results saved to: {args.output}")

    except Exception as e:
        logging.error(f"âŒ Error processing file: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
