#!/usr/bin/env python3
"""
ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿è¨ˆç®—ãƒ„ãƒ¼ãƒ«

PRFDatasetã®åˆæœŸåŒ–å‡¦ç†ã‚’å‚è€ƒã«ã—ã¦ã€ä»¥ä¸‹ã®å€¤ã‚’è¨ˆç®—ã—YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«å‡ºåŠ›ã™ã‚‹ï¼š
- src_vocab_list: ã‚½ãƒ¼ã‚¹èªå½™ãƒªã‚¹ãƒˆ
- tgt_vocab_list: ã‚¿ãƒ¼ã‚²ãƒƒãƒˆèªå½™ãƒªã‚¹ãƒˆ
- max_tgt_length: æœ€å¤§ã‚¿ãƒ¼ã‚²ãƒƒãƒˆé•·
- max_src_points: æœ€å¤§ã‚½ãƒ¼ã‚¹ç‚¹æ•°
"""

import argparse
import logging
import re
from typing import Any, Dict, List, Set, Tuple

import pandas as pd
import yaml
from tqdm import tqdm


def encode_expr(expr_str: str) -> List[str]:
    """exprã‚’ãƒˆãƒ¼ã‚¯ãƒ³åŒ–ï¼ˆã‚«ã‚¹ã‚¿ãƒ ãƒˆãƒ¼ã‚¯ãƒŠã‚¤ã‚¶ãƒ¼ã®å‡¦ç†ã‚’æ¨¡æ“¬ï¼‰"""
    # é–¢æ•°åï¼ˆZ,S,P,C,Rï¼‰ã€æ•°å­—ã€æ‹¬å¼§ã€ã‚«ãƒ³ãƒã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŠ½å‡º
    tokens = re.findall(r"[ZSPCRPRF]+|\d+|[\(\),\s]", expr_str)
    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
    tokens = [token for token in tokens if token.strip()]
    # BOS, EOSãƒˆãƒ¼ã‚¯ãƒ³ã‚’è¿½åŠ 
    return ["[BOS]"] + tokens + ["[EOS]"]


def calculate_all_metadata(
    csv_path: str,
) -> Tuple[Set[str], Set[str], int, int]:
    """
    CSVãƒ•ã‚¡ã‚¤ãƒ«ã‚’1å›ã ã‘èµ°æŸ»ã—ã¦å…¨ã¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—

    Returns:
        Tuple[src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points]
    """
    src_vocab = set()
    tgt_vocab = set()
    max_tgt_length = 0
    max_src_points = 0
    skipped_count = 0

    try:
        chunk_size = 1000

        # ãƒ•ã‚¡ã‚¤ãƒ«ã®ç·è¡Œæ•°ã‚’å–å¾—
        total_rows = sum(1 for _ in open(csv_path)) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã
        logging.info(f"Total rows to process: {total_rows}")

        with tqdm(
            total=total_rows,
            desc="ğŸ” Processing CSV data",
            unit="rows",
            bar_format="{l_bar}{bar}| {n_fmt}/{total_fmt} [{elapsed}<{remaining}, {rate_fmt}]",
        ) as pbar:
            for chunk in pd.read_csv(csv_path, chunksize=chunk_size):
                for _, row in chunk.iterrows():
                    # 1. srcèªå½™ã®æŠ½å‡ºï¼ˆinput/outputï¼‰
                    if "input" in row and pd.notna(row["input"]):
                        input_str = str(row["input"])
                        # æ•°å­—ã€æ‹¬å¼§ã€ã‚«ãƒ³ãƒã€ã‚¹ãƒšãƒ¼ã‚¹ã‚’æŠ½å‡º
                        tokens = re.findall(r"\d+|[\[\],\(\)\s]", input_str)
                        src_vocab.update(tokens)

                        # 3. max_src_pointsã®è¨ˆç®—
                        try:
                            input_data = (
                                eval(input_str)
                                if isinstance(input_str, str)
                                else input_str
                            )
                            points = len(input_data)
                            max_src_points = max(max_src_points, points)
                        except (ValueError, SyntaxError, TypeError) as e:
                            skipped_count += 1
                            logging.debug(
                                f"Skipping input data due to error: {e}"
                            )

                    if "output" in row and pd.notna(row["output"]):
                        output_str = str(row["output"])
                        tokens = re.findall(r"\d+|[\[\],\(\)\s]", output_str)
                        src_vocab.update(tokens)

                    # 2. tgtèªå½™ã®æŠ½å‡ºã¨max_tgt_lengthã®è¨ˆç®—ï¼ˆexprï¼‰
                    if "expr" in row and pd.notna(row["expr"]):
                        expr_str = str(row["expr"])

                        # tgtèªå½™ã®æŠ½å‡º
                        tokens = re.findall(
                            r"[ZSPCRPRF]+|\d+|[\(\),\s]", expr_str
                        )
                        tgt_vocab.update(tokens)

                        # max_tgt_lengthã®è¨ˆç®—
                        encoded_tokens = encode_expr(expr_str)
                        max_tgt_length = max(
                            max_tgt_length, len(encoded_tokens)
                        )

                    pbar.update(1)

                    # çµ±è¨ˆæƒ…å ±ã‚’100è¡Œã”ã¨ã«æ›´æ–°
                    if pbar.n % 100 == 0:
                        pbar.set_postfix(
                            src_vocab=len(src_vocab),
                            tgt_vocab=len(tgt_vocab),
                            max_tgt_len=max_tgt_length,
                            max_src_pts=max_src_points,
                            skipped=skipped_count,
                            refresh=False,
                        )

    except Exception as e:
        logging.warning(f"Chunk reading failed: {e}. Trying normal reading...")
        # ãƒãƒ£ãƒ³ã‚¯èª­ã¿è¾¼ã¿ãŒå¤±æ•—ã—ãŸå ´åˆã¯é€šå¸¸ã®èª­ã¿è¾¼ã¿
        df = pd.read_csv(csv_path)

        logging.info(f"Fallback: Processing {len(df)} rows in single batch")

        for _, row in tqdm(
            df.iterrows(),
            total=len(df),
            desc="ğŸ” Processing CSV data (fallback)",
            unit="rows",
        ):
            # åŒã˜å‡¦ç†ã‚’ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯ç”¨ã«å®Ÿè¡Œ
            if "input" in row and pd.notna(row["input"]):
                input_str = str(row["input"])
                tokens = re.findall(r"\d+|[\[\],\(\)\s]", input_str)
                src_vocab.update(tokens)

                try:
                    input_data = (
                        eval(input_str)
                        if isinstance(input_str, str)
                        else input_str
                    )
                    points = len(input_data)
                    max_src_points = max(max_src_points, points)
                except (ValueError, SyntaxError, TypeError) as e:
                    skipped_count += 1
                    logging.debug(f"Skipping input data due to error: {e}")

            if "output" in row and pd.notna(row["output"]):
                output_str = str(row["output"])
                tokens = re.findall(r"\d+|[\[\],\(\)\s]", output_str)
                src_vocab.update(tokens)

            if "expr" in row and pd.notna(row["expr"]):
                expr_str = str(row["expr"])
                tokens = re.findall(r"[ZSPCRPRF]+|\d+|[\(\),\s]", expr_str)
                tgt_vocab.update(tokens)

                encoded_tokens = encode_expr(expr_str)
                max_tgt_length = max(max_tgt_length, len(encoded_tokens))

    # ç©ºæ–‡å­—åˆ—ã‚’é™¤å»
    src_vocab.discard("")
    tgt_vocab.discard("")

    # ãƒ‡ãƒ•ã‚©ãƒ«ãƒˆå€¤ã®è¨­å®š
    if max_tgt_length == 0:
        max_tgt_length = 128
    if max_src_points == 0:
        max_src_points = 20

    if skipped_count > 0:
        logging.warning(f"Skipped {skipped_count} samples during processing")

    logging.info("Processing completed:")
    logging.info(f"  - Src vocabulary size: {len(src_vocab)}")
    logging.info(f"  - Tgt vocabulary size: {len(tgt_vocab)}")
    logging.info(f"  - Max target length: {max_tgt_length}")
    logging.info(f"  - Max source points: {max_src_points}")

    return src_vocab, tgt_vocab, max_tgt_length, max_src_points


def main():
    """ãƒ¡ã‚¤ãƒ³é–¢æ•°"""
    parser = argparse.ArgumentParser(
        description="Calculate metadata from CSV file and output to YAML"
    )
    parser.add_argument(
        "-i", "--input", required=True, help="Input CSV file path"
    )
    parser.add_argument(
        "-o", "--output", required=True, help="Output YAML file path"
    )
    parser.add_argument(
        "-v", "--verbose", action="store_true", help="Enable verbose logging"
    )

    args = parser.parse_args()

    # ãƒ­ã‚°ãƒ¬ãƒ™ãƒ«ã®è¨­å®š
    log_level = logging.DEBUG if args.verbose else logging.INFO
    logging.basicConfig(
        level=log_level, format="%(asctime)s - %(levelname)s - %(message)s"
    )

    try:
        print(f"ğŸ” Processing CSV file: {args.input}")

        # ãƒ•ã‚¡ã‚¤ãƒ«ã‚µã‚¤ã‚ºã¨ãƒ¬ã‚³ãƒ¼ãƒ‰æ•°ã®äº‹å‰ç¢ºèª
        import os

        file_size_mb = os.path.getsize(args.input) / (1024 * 1024)
        total_rows = sum(1 for _ in open(args.input)) - 1  # ãƒ˜ãƒƒãƒ€ãƒ¼ã‚’é™¤ã

        print(f"ğŸ“Š File info: {file_size_mb:.2f} MB, {total_rows:,} rows")
        print("\nğŸš€ Starting single-pass metadata calculation...")

        # 1å›ã®èµ°æŸ»ã§å…¨ã¦ã®ãƒ¡ã‚¿ãƒ‡ãƒ¼ã‚¿ã‚’è¨ˆç®—
        src_vocab_set, tgt_vocab_set, max_tgt_length, max_src_points = (
            calculate_all_metadata(args.input)
        )

        print("\nğŸ”„ Converting vocabularies to sorted lists...")
        # ãƒªã‚¹ãƒˆã«å¤‰æ›ã—ã¦ã‚½ãƒ¼ãƒˆ
        src_vocab_list = sorted(list(src_vocab_set))
        tgt_vocab_list = sorted(list(tgt_vocab_set))

        # YAMLã«å‡ºåŠ›ã™ã‚‹ãƒ‡ãƒ¼ã‚¿ã‚’ä½œæˆï¼ˆ2ã¤ã®listãŒæœ€å¾Œã«ãªã‚‹ä¸¦ã³ï¼‰
        metadata = {
            "max_tgt_length": max_tgt_length,
            "max_src_points": max_src_points,
            "src_vocab_list": src_vocab_list,
            "tgt_vocab_list": tgt_vocab_list,
        }

        # YAMLãƒ•ã‚¡ã‚¤ãƒ«ã«æ›¸ãå‡ºã—
        print(f"ğŸ’¾ Writing metadata to: {args.output}")
        with open(args.output, "w", encoding="utf-8") as f:
            yaml.dump(metadata, f, default_flow_style=False, allow_unicode=True)

        # çµæœã®ã‚µãƒãƒªãƒ¼ã‚’è¡¨ç¤º
        print("\nâœ… Metadata calculation completed!")
        print("=" * 50)
        print(f"ğŸ“ Max target length:     {max_tgt_length:,}")
        print(f"ğŸ“Š Max source points:     {max_src_points:,}")
        print(f"ğŸ“š Source vocabulary size: {len(src_vocab_list):,}")
        print(f"ğŸ¯ Target vocabulary size: {len(tgt_vocab_list):,}")
        print("=" * 50)
        print(f"ğŸ’¾ Results saved to: {args.output}")

    except Exception as e:
        logging.error(f"âŒ Error processing file: {e}")
        import traceback

        traceback.print_exc()
        return 1

    return 0


if __name__ == "__main__":
    exit(main())
