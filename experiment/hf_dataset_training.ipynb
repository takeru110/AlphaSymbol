{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf9c7f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path: /Users/takeruito/work/PrfSR\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "root_path = Path(\".\").resolve().parent\n",
    "print(\"Root path:\", root_path)\n",
    "sys.path.append(str(root_path))\n",
    "\n",
    "import importlib\n",
    "from src.model_meta.dataset import CSVDataModule, custom_collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0bea7",
   "metadata": {},
   "source": [
    "# Load Config path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4d863d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata path: /home/takeru/AlphaSymbol/data/training/mini_metadata.yaml\n",
      "max_epoch: 1000\n",
      "max_value: 2000\n",
      "min_n_tokens_in_batch: 2000\n",
      "test_ratio: 0.1\n",
      "val_ratio: 0.25\n",
      "num_workers: 13\n",
      "token_embed_dim: 16\n",
      "emb_expansion_factor: 1\n",
      "learning_rate: 3*10**(-4)\n",
      "nhead: 16\n",
      "num_encoder_layers: 4\n",
      "num_decoder_layers: 6\n",
      "dim_feedforward: 512\n",
      "dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_config_path = root_path / \"src/model_meta/training_config.yaml\"\n",
    "assert Path(train_config_path).exists(), FileNotFoundError(f\"Train config file not found: {train_config_path}\")\n",
    "with open(train_config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(f\"Metadata path: {config[\"metadata_path\"]:}\")\n",
    "print(f\"max_epoch: {config['max_epoch']}\")\n",
    "print(f\"max_value: {config['max_value']}\")\n",
    "print(f\"min_n_tokens_in_batch: {config['min_n_tokens_in_batch']}\")\n",
    "print(f\"test_ratio: {config['test_ratio']}\")\n",
    "print(f\"val_ratio: {config['val_ratio']}\")\n",
    "print(f\"num_workers: {config['num_workers']}\")\n",
    "print(f\"token_embed_dim: {config['token_embed_dim']}\")\n",
    "print(f\"emb_expansion_factor: {config['emb_expansion_factor']}\")\n",
    "print(f\"learning_rate: {config['learning_rate']}\")\n",
    "print(f\"nhead: {config['transformer']['nhead']}\")\n",
    "print(f\"num_encoder_layers: {config['transformer']['num_encoder_layers']}\")\n",
    "print(f\"num_decoder_layers: {config[\"transformer\"][\"num_decoder_layers\"]}\")\n",
    "print(f\"dim_feedforward: {config[\"transformer\"][\"dim_feedforward\"]}\")\n",
    "print(f\"dropout: {config[\"transformer\"][\"dropout\"]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30415c87",
   "metadata": {},
   "source": [
    "# Load CSV by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a309bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 0 - Rows: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 8101.42 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 0 with size 10000\n",
      "Train dataset: 9000 samples\n",
      "Validation dataset: 1000 samples\n",
      "MiniData 1 - Source shape: torch.Size([64, 10, 4]), Target shape: torch.Size([64, 124])\n",
      "\n",
      "Chunk 1 - Rows: 10000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10000/10000 [00:01<00:00, 7679.37 examples/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 1 with size 10000\n",
      "Train dataset: 9000 samples\n",
      "Validation dataset: 1000 samples\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[14]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     15\u001b[39m dataloader = CSVDataModule(\n\u001b[32m     16\u001b[39m     dataset=dataset,\n\u001b[32m     17\u001b[39m     batch_size=config[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m   (...)\u001b[39m\u001b[32m     24\u001b[39m     max_batch_size=config[\u001b[33m'\u001b[39m\u001b[33mbatch_size\u001b[39m\u001b[33m'\u001b[39m],\n\u001b[32m     25\u001b[39m )\n\u001b[32m     27\u001b[39m dataloader.setup()\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m batch = \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(\u001b[43mdataloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m     29\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMiniData \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_idx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Source shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Target shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;28mprint\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/src/model_meta/dataset.py:151\u001b[39m, in \u001b[36mCSVDataModule.train_dataloader\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    149\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"学習用DataLoader\"\"\"\u001b[39;00m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.batching_strategy == \u001b[33m\"\u001b[39m\u001b[33mlength_aware_token\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m151\u001b[39m     batch_sampler = \u001b[43mLengthAwareTokenBatchSampler\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    152\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmin_tokens_per_batch\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmin_tokens_per_batch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmax_batch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    155\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m DataLoader(\n\u001b[32m    157\u001b[39m         \u001b[38;5;28mself\u001b[39m.train_dataset,\n\u001b[32m    158\u001b[39m         batch_sampler=batch_sampler,\n\u001b[32m   (...)\u001b[39m\u001b[32m    161\u001b[39m         pin_memory=\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    162\u001b[39m     )\n\u001b[32m    163\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    164\u001b[39m     \u001b[38;5;66;03m# デフォルトの固定バッチサイズ\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/src/model_meta/dataset.py:39\u001b[39m, in \u001b[36mLengthAwareTokenBatchSampler.__init__\u001b[39m\u001b[34m(self, dataset, max_batch_size, min_tokens_per_batch)\u001b[39m\n\u001b[32m     37\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_lengths = []\n\u001b[32m     38\u001b[39m length_counter = \u001b[32m1\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43midx\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m:\u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mcontinue\u001b[39;49;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2394\u001b[39m, in \u001b[36mDataset.__iter__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2392\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   2393\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m.num_rows):\n\u001b[32m-> \u001b[39m\u001b[32m2394\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_getitem\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2395\u001b[39m \u001b[43m            \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2396\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/arrow_dataset.py:2787\u001b[39m, in \u001b[36mDataset._getitem\u001b[39m\u001b[34m(self, key, **kwargs)\u001b[39m\n\u001b[32m   2785\u001b[39m format_kwargs = format_kwargs \u001b[38;5;28;01mif\u001b[39;00m format_kwargs \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[32m   2786\u001b[39m formatter = get_formatter(format_type, features=\u001b[38;5;28mself\u001b[39m._info.features, **format_kwargs)\n\u001b[32m-> \u001b[39m\u001b[32m2787\u001b[39m pa_subtable = \u001b[43mquery_table\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_indices\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mis\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2788\u001b[39m formatted_output = format_table(\n\u001b[32m   2789\u001b[39m     pa_subtable, key, formatter=formatter, format_columns=format_columns, output_all_columns=output_all_columns\n\u001b[32m   2790\u001b[39m )\n\u001b[32m   2791\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m formatted_output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:588\u001b[39m, in \u001b[36mquery_table\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m    586\u001b[39m     pa_subtable = _query_table(table, key)\n\u001b[32m    587\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m588\u001b[39m     pa_subtable = \u001b[43m_query_table_with_indices_mapping\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    589\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa_subtable\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/formatting/formatting.py:60\u001b[39m, in \u001b[36m_query_table_with_indices_mapping\u001b[39m\u001b[34m(table, key, indices)\u001b[39m\n\u001b[32m     53\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     54\u001b[39m \u001b[33;03mQuery a pyarrow Table to extract the subtable that correspond to the given key.\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[33;03mThe :obj:`indices` parameter corresponds to the indices mapping in case we cant to take into\u001b[39;00m\n\u001b[32m     56\u001b[39m \u001b[33;03maccount a shuffling or an indices selection for example.\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03mThe indices table must contain one column named \"indices\" of type uint64.\u001b[39;00m\n\u001b[32m     58\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     59\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mint\u001b[39m):\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m     key = \u001b[43mindices\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfast_slice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m \u001b[49m\u001b[43m%\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnum_rows\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m.column(\u001b[32m0\u001b[39m)[\u001b[32m0\u001b[39m].as_py()\n\u001b[32m     61\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _query_table(table, key)\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, \u001b[38;5;28mslice\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:161\u001b[39m, in \u001b[36mIndexedTableMixin.fast_slice\u001b[39m\u001b[34m(self, offset, length)\u001b[39m\n\u001b[32m    159\u001b[39m     batches = \u001b[38;5;28mself\u001b[39m._batches[i : j + \u001b[32m1\u001b[39m]\n\u001b[32m    160\u001b[39m     batches[-\u001b[32m1\u001b[39m] = batches[-\u001b[32m1\u001b[39m].slice(\u001b[32m0\u001b[39m, offset + length - \u001b[38;5;28mself\u001b[39m._offsets[j])\n\u001b[32m--> \u001b[39m\u001b[32m161\u001b[39m     batches[\u001b[32m0\u001b[39m] = \u001b[43mbatches\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m.\u001b[49m\u001b[43mslice\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_offsets\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    162\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m pa.Table.from_batches(batches, schema=\u001b[38;5;28mself\u001b[39m._schema)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "dataset_path = \"/Users/takeruito/work/PrfSR/data/training/superfib_r1_dataset.csv\"\n",
    "chunk_reader = pd.read_csv(dataset_path, chunksize=10000)\n",
    "\n",
    "for chunk_idx, chunk_df in enumerate(chunk_reader):\n",
    "    print(f\"Chunk {chunk_idx} - Rows: {len(chunk_df)}\")\n",
    "    dataset = Dataset.from_pandas(chunk_df)\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\n",
    "            \"source\": eval(x[\"source\"]),\n",
    "            \"target\": eval(x[\"target\"]),\n",
    "        }\n",
    "    )\n",
    "    print(f\"Processing shard {chunk_idx} with size {len(chunk_df)}\")\n",
    "    \n",
    "    dataloader = CSVDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=0, # config['num_workers'],\n",
    "        train_val_split=1 - config[\"test_ratio\"],\n",
    "        seed=42,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        batching_strategy=config['batching_strategy'],\n",
    "        min_tokens_per_batch=config['min_n_tokens_in_batch'],\n",
    "        max_batch_size=config['batch_size'],\n",
    "    )\n",
    "    \n",
    "    dataloader.setup()\n",
    "    batch = next(iter(dataloader.train_dataloader()))\n",
    "    print(f\"MiniData {chunk_idx+1} - Source shape: {batch['source'].shape}, Target shape: {batch['target'].shape}\")\n",
    "    print()\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
