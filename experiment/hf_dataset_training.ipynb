{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cf9c7f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ OmegaConf imported successfully\n",
      "üìÅ Root path: /Users/takeruito/work/PrfSR\n",
      "üöÄ All imports successful!\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk, Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import yaml\n",
    "import json\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "# OmegaConf for configuration management (Hydra-style)\n",
    "try:\n",
    "    from omegaconf import OmegaConf\n",
    "    print(\"‚úÖ OmegaConf imported successfully\")\n",
    "except ImportError:\n",
    "    print(\"‚ùå OmegaConf not found. Install with: pip install omegaconf\")\n",
    "    raise\n",
    "\n",
    "root_path = Path.cwd().parent\n",
    "print(\"üìÅ Root path:\", root_path)\n",
    "sys.path.append(str(root_path))\n",
    "\n",
    "import lightning as pl\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from lightning.pytorch.loggers import WandbLogger\n",
    "\n",
    "from src.model_meta.dataset import CSVDataModule, custom_collate_fn\n",
    "from src.model_meta.train import LitTransformer\n",
    "\n",
    "print(\"üöÄ All imports successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b0bea7",
   "metadata": {},
   "source": [
    "# Load Config path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d863d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Configuration loaded with OmegaConf:\n",
      "Metadata path: /Users/takeruito/work/PrfSR/data/training/superfib_r1_metadata.yaml\n",
      "Data path: /Users/takeruito/work/PrfSR/data/training/superfib_r1_dataset.csv\n",
      "max_epoch: 1000\n",
      "max_value: 2000\n",
      "min_n_tokens_in_batch: 2000\n",
      "test_ratio: 0.1\n",
      "val_ratio: 0.25\n",
      "num_workers: 13\n",
      "token_embed_dim: 16\n",
      "emb_expansion_factor: 1\n",
      "learning_rate: 3*10**(-4)\n",
      "batch_size: 64\n",
      "batching_strategy: length_aware_token\n",
      "\n",
      "ü§ñ Transformer config:\n",
      "nhead: 16\n",
      "num_encoder_layers: 4\n",
      "num_decoder_layers: 6\n",
      "dim_feedforward: 512\n",
      "dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "# Load config using OmegaConf (Hydra-style)\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "train_config_path = root_path / \"src/model_meta/training_config.yaml\"\n",
    "assert Path(train_config_path).exists(), FileNotFoundError(f\"Train config file not found: {train_config_path}\")\n",
    "\n",
    "# Load with OmegaConf for dot notation access\n",
    "config = OmegaConf.load(train_config_path)\n",
    "\n",
    "print(f\"üîß Configuration loaded with OmegaConf:\")\n",
    "print(f\"Metadata path: {config.metadata_path}\")\n",
    "print(f\"Data path: {config.data_path}\")\n",
    "print(f\"max_epoch: {config.max_epoch}\")\n",
    "print(f\"max_value: {config.max_value}\")\n",
    "print(f\"min_n_tokens_in_batch: {config.min_n_tokens_in_batch}\")\n",
    "print(f\"test_ratio: {config.test_ratio}\")\n",
    "print(f\"val_ratio: {config.val_ratio}\")\n",
    "print(f\"num_workers: {config.num_workers}\")\n",
    "print(f\"token_embed_dim: {config.token_embed_dim}\")\n",
    "print(f\"emb_expansion_factor: {config.emb_expansion_factor}\")\n",
    "print(f\"learning_rate: {config.learning_rate}\")\n",
    "print(f\"batch_size: {config.batch_size}\")\n",
    "print(f\"batching_strategy: {config.batching_strategy}\")\n",
    "\n",
    "print(f\"\\nü§ñ Transformer config:\")\n",
    "print(f\"nhead: {config.transformer.nhead}\")\n",
    "print(f\"num_encoder_layers: {config.transformer.num_encoder_layers}\")\n",
    "print(f\"num_decoder_layers: {config.transformer.num_decoder_layers}\")\n",
    "print(f\"dim_feedforward: {config.transformer.dim_feedforward}\")\n",
    "print(f\"dropout: {config.transformer.dropout}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd671109",
   "metadata": {},
   "source": [
    "# Load metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "23ca7ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Metadata loaded:\n",
      "max_src_points: 80\n",
      "max_tgt_length: 859\n",
      "max_point_dim: 4\n",
      "src_vocab_size: 1002\n",
      "tgt_vocab_size: 15\n",
      "\n",
      "üî§ Source vocabulary (first 10): ['[PAD]', '0', '1', '10', '100', '1000', '101', '102', '103', '104']\n",
      "üéØ Target vocabulary (first 10): ['[PAD]', '[BOS]', '[EOS]', '(', ')', ',', '1', '2', '3', '4']\n",
      "\n",
      "‚úÖ Inverse vocabularies created:\n",
      "src '[PAD]' index: 0\n",
      "tgt '[PAD]' index: 0\n",
      "tgt '[BOS]' index: 1\n",
      "tgt '[EOS]' index: 2\n"
     ]
    }
   ],
   "source": [
    "from omegaconf import OmegaConf\n",
    "import yaml\n",
    "\n",
    "# Load metadata using OmegaConf\n",
    "metadata_path = config.metadata_path\n",
    "metadata = OmegaConf.load(metadata_path)\n",
    "\n",
    "print(f\"üìä Metadata loaded:\")\n",
    "print(f\"max_src_points: {metadata.max_src_points}\")\n",
    "print(f\"max_tgt_length: {metadata.max_tgt_length}\")\n",
    "print(f\"max_point_dim: {metadata.max_point_dim}\")\n",
    "print(f\"src_vocab_size: {len(metadata.src_vocab_list)}\")\n",
    "print(f\"tgt_vocab_size: {len(metadata.tgt_vocab_list)}\")\n",
    "\n",
    "# Create inverse vocabularies more efficiently\n",
    "src_inv_vocab = {token: idx for idx, token in enumerate(metadata.src_vocab_list)}\n",
    "tgt_inv_vocab = {token: idx for idx, token in enumerate(metadata.tgt_vocab_list)}\n",
    "\n",
    "print(f\"\\nüî§ Source vocabulary (first 10): {list(src_inv_vocab.keys())[:10]}\")\n",
    "print(f\"üéØ Target vocabulary (first 10): {list(tgt_inv_vocab.keys())[:10]}\")\n",
    "\n",
    "# Add inverse vocabularies to metadata\n",
    "metadata.src_inv_vocab = src_inv_vocab\n",
    "metadata.tgt_inv_vocab = tgt_inv_vocab\n",
    "\n",
    "print(f\"\\n‚úÖ Inverse vocabularies created:\")\n",
    "print(f\"src '[PAD]' index: {metadata.src_inv_vocab['[PAD]']}\")\n",
    "print(f\"tgt '[PAD]' index: {metadata.tgt_inv_vocab['[PAD]']}\")\n",
    "print(f\"tgt '[BOS]' index: {metadata.tgt_inv_vocab['[BOS]']}\")\n",
    "print(f\"tgt '[EOS]' index: {metadata.tgt_inv_vocab['[EOS]']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30415c87",
   "metadata": {},
   "source": [
    "# Load CSV by chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a309bbaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/transformer.py:382: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(\n",
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ Dataset path: /Users/takeruito/work/PrfSR/data/training/superfib_r1_dataset.csv\n",
      "üß† Model parameters:\n",
      "  - src_token_num: 80\n",
      "  - tgt_token_num: 859\n",
      "  - token_embed_dim: 16\n",
      "  - max_src_dim: 4\n",
      "  - max_tgt_dim: 15\n",
      "\n",
      "üöÄ Starting chunked training...\n",
      "\n",
      "üì¶ Chunk 1 - Rows: 10,000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 10000/10000 [00:01<00:00, 7945.24 examples/s]\n",
      "\n",
      "  | Name          | Type               | Params\n",
      "-----------------------------------------------------\n",
      "0 | src_embedding | Embedding          | 1.3 K \n",
      "1 | fc1           | Linear             | 4.2 K \n",
      "2 | fc2           | Linear             | 4.2 K \n",
      "3 | tgt_embedding | Embedding          | 55.0 K\n",
      "4 | pos_enc       | PositionalEncoding | 0     \n",
      "5 | transformer   | Transformer        | 930 K \n",
      "6 | fc_out        | Linear             | 55.8 K\n",
      "7 | loss_fn       | CrossEntropyLoss   | 0     \n",
      "-----------------------------------------------------\n",
      "1.1 M     Trainable params\n",
      "0         Non-trainable params\n",
      "1.1 M     Total params\n",
      "4.205     Total estimated model params size (MB)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üî• Training on chunk 1...\n",
      "Train dataset: 9000 samples\n",
      "Validation dataset: 1000 samples\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:224: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 14 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  rank_zero_warn(\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (138) must match the size of tensor b (15) at non-singleton dimension 0",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 83\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;66;03m# Training\u001b[39;00m\n\u001b[32m     82\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33müî• Training on chunk \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mchunk_idx+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m83\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Log results\u001b[39;00m\n\u001b[32m     86\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(trainer, \u001b[33m'\u001b[39m\u001b[33mlogged_metrics\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m trainer.logged_metrics:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:603\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    601\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m`Trainer.fit()` requires a `LightningModule`, got: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.\u001b[34m__class__\u001b[39m.\u001b[34m__qualname__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    602\u001b[39m \u001b[38;5;28mself\u001b[39m.strategy._lightning_module = model\n\u001b[32m--> \u001b[39m\u001b[32m603\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    604\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    605\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/call.py:38\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     36\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m     37\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     40\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[32m     41\u001b[39m     trainer._call_teardown_hook()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:645\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    638\u001b[39m ckpt_path = ckpt_path \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.resume_from_checkpoint\n\u001b[32m    639\u001b[39m \u001b[38;5;28mself\u001b[39m._ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._set_ckpt_path(\n\u001b[32m    640\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    641\u001b[39m     ckpt_path,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    642\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    643\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    644\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m645\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    647\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n\u001b[32m    648\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1098\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1094\u001b[39m \u001b[38;5;28mself\u001b[39m._checkpoint_connector.restore_training_state()\n\u001b[32m   1096\u001b[39m \u001b[38;5;28mself\u001b[39m._checkpoint_connector.resume_end()\n\u001b[32m-> \u001b[39m\u001b[32m1098\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1100\u001b[39m log.detail(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: trainer tearing down\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1101\u001b[39m \u001b[38;5;28mself\u001b[39m._teardown()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1177\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1175\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.predicting:\n\u001b[32m   1176\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._run_predict()\n\u001b[32m-> \u001b[39m\u001b[32m1177\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1190\u001b[39m, in \u001b[36mTrainer._run_train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1187\u001b[39m \u001b[38;5;28mself\u001b[39m._pre_training_routine()\n\u001b[32m   1189\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m isolate_rng():\n\u001b[32m-> \u001b[39m\u001b[32m1190\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_sanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1192\u001b[39m \u001b[38;5;66;03m# enable train mode\u001b[39;00m\n\u001b[32m   1193\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.model \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1262\u001b[39m, in \u001b[36mTrainer._run_sanity_check\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1260\u001b[39m \u001b[38;5;66;03m# run eval step\u001b[39;00m\n\u001b[32m   1261\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m-> \u001b[39m\u001b[32m1262\u001b[39m     \u001b[43mval_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1264\u001b[39m \u001b[38;5;28mself\u001b[39m._call_callback_hooks(\u001b[33m\"\u001b[39m\u001b[33mon_sanity_check_end\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1266\u001b[39m \u001b[38;5;66;03m# reset logger connector\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/loop.py:199\u001b[39m, in \u001b[36mLoop.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    201\u001b[39m     \u001b[38;5;28mself\u001b[39m._restarting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/dataloader/evaluation_loop.py:152\u001b[39m, in \u001b[36mEvaluationLoop.advance\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    150\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.num_dataloaders > \u001b[32m1\u001b[39m:\n\u001b[32m    151\u001b[39m     kwargs[\u001b[33m\"\u001b[39m\u001b[33mdataloader_idx\u001b[39m\u001b[33m\"\u001b[39m] = dataloader_idx\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m dl_outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdl_max_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    154\u001b[39m \u001b[38;5;66;03m# store batch level output per dataloader\u001b[39;00m\n\u001b[32m    155\u001b[39m \u001b[38;5;28mself\u001b[39m._outputs.append(dl_outputs)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/loop.py:199\u001b[39m, in \u001b[36mLoop.run\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    197\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    198\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_start(*args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m199\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    200\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end()\n\u001b[32m    201\u001b[39m     \u001b[38;5;28mself\u001b[39m._restarting = \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/epoch/evaluation_epoch_loop.py:137\u001b[39m, in \u001b[36mEvaluationEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher, dl_max_batches, kwargs)\u001b[39m\n\u001b[32m    134\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_started()\n\u001b[32m    136\u001b[39m \u001b[38;5;66;03m# lightning module methods\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_evaluation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m output = \u001b[38;5;28mself\u001b[39m._evaluation_step_end(output)\n\u001b[32m    140\u001b[39m \u001b[38;5;28mself\u001b[39m.batch_progress.increment_processed()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/loops/epoch/evaluation_epoch_loop.py:234\u001b[39m, in \u001b[36mEvaluationEpochLoop._evaluation_step\u001b[39m\u001b[34m(self, **kwargs)\u001b[39m\n\u001b[32m    223\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"The evaluation step (validation_step or test_step depending on the trainer's state).\u001b[39;00m\n\u001b[32m    224\u001b[39m \n\u001b[32m    225\u001b[39m \u001b[33;03mArgs:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    231\u001b[39m \u001b[33;03m    the outputs of the step\u001b[39;00m\n\u001b[32m    232\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    233\u001b[39m hook_name = \u001b[33m\"\u001b[39m\u001b[33mtest_step\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.trainer.testing \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mvalidation_step\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m234\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhook_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    236\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/trainer/trainer.py:1480\u001b[39m, in \u001b[36mTrainer._call_strategy_hook\u001b[39m\u001b[34m(self, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m   1477\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1479\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m-> \u001b[39m\u001b[32m1480\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1482\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[32m   1483\u001b[39m pl_module._current_fx_name = prev_fx_name\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/lightning/pytorch/strategies/strategy.py:390\u001b[39m, in \u001b[36mStrategy.validation_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    388\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m.precision_plugin.val_step_context():\n\u001b[32m    389\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.model, ValidationStep)\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidation_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/src/model_meta/train.py:130\u001b[39m, in \u001b[36mLitTransformer.validation_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m    128\u001b[39m tgt_input = tgt_batch[:, :-\u001b[32m1\u001b[39m]\n\u001b[32m    129\u001b[39m tgt_output = tgt_batch[:, \u001b[32m1\u001b[39m:]\n\u001b[32m--> \u001b[39m\u001b[32m130\u001b[39m output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msrc_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_input\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# (T, N, C)\u001b[39;00m\n\u001b[32m    131\u001b[39m output = output.permute(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m, \u001b[32m0\u001b[39m)  \u001b[38;5;66;03m# (N, C, T)\u001b[39;00m\n\u001b[32m    132\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.loss_fn(output, tgt_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/src/model_meta/train.py:105\u001b[39m, in \u001b[36mLitTransformer.forward\u001b[39m\u001b[34m(self, src_input, tgt_input)\u001b[39m\n\u001b[32m    101\u001b[39m tgt_embeddings = \u001b[38;5;28mself\u001b[39m.tgt_embedding(tgt_input) * torch.sqrt(\n\u001b[32m    102\u001b[39m     torch.tensor(\u001b[38;5;28mself\u001b[39m.hidden_size, dtype=torch.float32)\n\u001b[32m    103\u001b[39m )\n\u001b[32m    104\u001b[39m tgt_embeddings = tgt_embeddings.permute(\u001b[32m1\u001b[39m, \u001b[32m0\u001b[39m, \u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m105\u001b[39m tgt_embeddings = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpos_enc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtgt_embeddings\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    107\u001b[39m tgt_mask = nn.Transformer.generate_square_subsequent_mask(\n\u001b[32m    108\u001b[39m     tgt_embeddings.size(\u001b[32m0\u001b[39m)\n\u001b[32m    109\u001b[39m )\n\u001b[32m    110\u001b[39m output = \u001b[38;5;28mself\u001b[39m.transformer(\n\u001b[32m    111\u001b[39m     src_embeddings, tgt_embeddings, tgt_mask=tgt_mask\n\u001b[32m    112\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1751\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1749\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1750\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1751\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1762\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1760\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1761\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1764\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1765\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/src/model_meta/models.py:28\u001b[39m, in \u001b[36mPositionalEncoding.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     24\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     25\u001b[39m \u001b[33;03mx: Tensor of shape (seq_len, batch_size, d_model)\u001b[39;00m\n\u001b[32m     26\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m     27\u001b[39m seq_len = x.size(\u001b[32m0\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpe\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mseq_len\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (138) must match the size of tensor b (15) at non-singleton dimension 0"
     ]
    }
   ],
   "source": [
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='val_loss',\n",
    "    save_top_k=3,\n",
    "    mode='min',\n",
    "    dirpath='./checkpoints',\n",
    "    filename='model-{epoch:02d}-{val_loss:.2f}'\n",
    ")\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=5,\n",
    "    mode='min',\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "# Initialize variables\n",
    "dataset_path = config.data_path\n",
    "total_samples = 0\n",
    "losses = []\n",
    "\n",
    "print(f\"üìÅ Dataset path: {dataset_path}\")\n",
    "print(f\"üß† Model parameters:\")\n",
    "print(f\"  - src_token_num: {metadata.max_src_points}\")\n",
    "print(f\"  - tgt_token_num: {len(metadata.tgt_vocab_list)}\")\n",
    "print(f\"  - token_embed_dim: {config.token_embed_dim}\")\n",
    "print(f\"  - max_src_dim: {metadata.max_point_dim}\")\n",
    "print(f\"  - max_tgt_dim: {metadata.max_tgt_length}\")\n",
    "\n",
    "chunk_reader = pd.read_csv(dataset_path, chunksize=10000)\n",
    "model = LitTransformer(\n",
    "    src_token_num=metadata.max_src_points,\n",
    "    tgt_token_num=len(metadata.tgt_vocab_list),\n",
    "    token_embed_dim=config.token_embed_dim,\n",
    "    max_src_dim=metadata.max_point_dim,\n",
    "    max_tgt_dim=metadata.max_tgt_length,\n",
    "    src_padding_idx=metadata.src_inv_vocab['[PAD]'],\n",
    "    tgt_padding_idx=metadata.tgt_inv_vocab['[PAD]'],\n",
    "    emb_expansion_factor=config.emb_expansion_factor,\n",
    "    t_config=config.transformer,  # Now this will work with dot notation in train.py\n",
    "    learning_rate=eval(config.learning_rate),\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=config.max_epoch,\n",
    "    accelerator='auto',\n",
    "    devices=1,\n",
    "    callbacks=[checkpoint_callback, early_stopping],\n",
    "    logger=False,\n",
    "    enable_progress_bar=True\n",
    ")\n",
    "\n",
    "print(f\"\\nüöÄ Starting chunked training...\")\n",
    "\n",
    "for chunk_idx, chunk_df in enumerate(chunk_reader):\n",
    "    print(f\"\\nüì¶ Chunk {chunk_idx+1} - Rows: {len(chunk_df):,}\")\n",
    "    \n",
    "    # Convert DataFrame to Dataset\n",
    "    dataset = Dataset.from_pandas(chunk_df)\n",
    "    dataset = dataset.map(\n",
    "        lambda x: {\n",
    "            \"source\": eval(x[\"source\"]),\n",
    "            \"target\": eval(x[\"target\"]),\n",
    "        },\n",
    "        batched=False,\n",
    "        num_proc=1\n",
    "    )\n",
    "    \n",
    "    # Create DataModule\n",
    "    datamodule = CSVDataModule(\n",
    "        dataset=dataset,\n",
    "        batch_size=config.batch_size,\n",
    "        num_workers=0,  # config.num_workers,\n",
    "        train_val_split=1 - config.test_ratio,\n",
    "        seed=42,\n",
    "        collate_fn=custom_collate_fn,\n",
    "        batching_strategy=config.batching_strategy,\n",
    "        min_tokens_per_batch=config.min_n_tokens_in_batch,\n",
    "        max_batch_size=config.batch_size,\n",
    "    )\n",
    "    \n",
    "    # Training\n",
    "    print(f\"üî• Training on chunk {chunk_idx+1}...\")\n",
    "    trainer.fit(model, datamodule)\n",
    "    \n",
    "    # Log results\n",
    "    if hasattr(trainer, 'logged_metrics') and trainer.logged_metrics:\n",
    "        val_loss = trainer.logged_metrics.get('val_loss', 0)\n",
    "    elif hasattr(trainer, 'callback_metrics') and trainer.callback_metrics:\n",
    "        val_loss = trainer.callback_metrics.get('val_loss', 0)\n",
    "    else:\n",
    "        val_loss = 0.0\n",
    "    \n",
    "    total_samples += len(chunk_df)\n",
    "    losses.append(float(val_loss))\n",
    "    \n",
    "    print(f\"‚úÖ Chunk {chunk_idx+1} completed:\")\n",
    "    print(f\"  - Val Loss: {val_loss:.4f}\")\n",
    "    print(f\"  - Total samples processed: {total_samples:,}\")\n",
    "    \n",
    "    # Break after 3 chunks for testing\n",
    "    if chunk_idx >= 2:\n",
    "        print(\"üõë Stopping after 3 chunks for testing...\")\n",
    "        break\n",
    "\n",
    "print(f\"\\nüéâ Training completed!\")\n",
    "print(f\"üìä Total samples processed: {total_samples:,}\")\n",
    "print(f\"üìà Loss progression: {[f'{loss:.4f}' for loss in losses]}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
