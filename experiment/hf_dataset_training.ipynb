{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9c7f28",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root path: /Users/takeruito/work/PrfSR\n",
      "False\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset, load_from_disk\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "import pickle\n",
    "import yaml\n",
    "import json\n",
    "from collections import defaultdict\n",
    "root_path = Path(\".\").resolve().parent\n",
    "print(\"Root path:\", root_path)\n",
    "sys.path.append(str(root_path))\n",
    "\n",
    "import importlib\n",
    "from src.model_meta.dataset import CSVDataModule\n",
    "print(\"point_num_dict\" in dir(CSVDataModule))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92bbc80a",
   "metadata": {},
   "source": [
    "# Load from CSV and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f51329c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"/Users/takeruito/work/PrfSR/data/training/superfib_r1_dataset.csv\"\n",
    "dataset = load_dataset(\"csv\", data_files=dataset_path, split=\"train\")\n",
    "\n",
    "def formatter(sample):\n",
    "    sample[\"source\"] = eval(sample[\"source\"])\n",
    "    sample[\"target\"] = eval(sample[\"target\"])\n",
    "    return sample\n",
    "\n",
    "eval_dataset = dataset.map(formatter, batched=False, num_proc=10, load_from_cache_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362806b1",
   "metadata": {},
   "source": [
    "# Load from hf data directory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a6fbd71f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1421: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  table = cls._concat_blocks(blocks, axis=0)\n"
     ]
    }
   ],
   "source": [
    "eval_dataset = load_from_disk(\"/Users/takeruito/work/PrfSR/data/training/dir_hf_superfib_eval_dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95dbb9db",
   "metadata": {},
   "source": [
    "# Load config path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b35727da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata path: data/training/superfib_r1_metadata.pickle\n",
      "max_epoch: 3\n",
      "max_value: 2000\n",
      "min_n_tokens_in_batch: 2000\n",
      "test_ratio: 0.5\n",
      "val_ratio: 0.25\n",
      "num_workers: 16\n",
      "token_embed_dim: 16\n",
      "emb_expansion_factor: 1\n",
      "learning_rate: 3*10**(-4)\n",
      "nhead: 16\n",
      "num_encoder_layers: 4\n",
      "num_decoder_layers: 6\n",
      "dim_feedforward: 512\n",
      "dropout: 0.1\n"
     ]
    }
   ],
   "source": [
    "train_config_path = root_path / \"src/model_meta/training_config.yaml\"\n",
    "assert Path(train_config_path).exists(), FileNotFoundError(f\"Train config file not found: {train_config_path}\")\n",
    "with open(train_config_path, 'r') as f:\n",
    "    config = yaml.safe_load(f)\n",
    "print(f\"Metadata path: {config[\"metadata_path\"]:}\")\n",
    "print(f\"max_epoch: {config['max_epoch']}\")\n",
    "print(f\"max_value: {config['max_value']}\")\n",
    "print(f\"min_n_tokens_in_batch: {config['min_n_tokens_in_batch']}\")\n",
    "print(f\"test_ratio: {config['test_ratio']}\")\n",
    "print(f\"val_ratio: {config['val_ratio']}\")\n",
    "print(f\"num_workers: {config['num_workers']}\")\n",
    "print(f\"token_embed_dim: {config['token_embed_dim']}\")\n",
    "print(f\"emb_expansion_factor: {config['emb_expansion_factor']}\")\n",
    "print(f\"learning_rate: {config['learning_rate']}\")\n",
    "print(f\"nhead: {config['transformer']['nhead']}\")\n",
    "print(f\"num_encoder_layers: {config['transformer']['num_encoder_layers']}\")\n",
    "print(f\"num_decoder_layers: {config[\"transformer\"][\"num_decoder_layers\"]}\")\n",
    "print(f\"dim_feedforward: {config[\"transformer\"][\"dim_feedforward\"]}\")\n",
    "print(f\"dropout: {config[\"transformer\"][\"dropout\"]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50858d51",
   "metadata": {},
   "source": [
    "# Load Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "216c61b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“‹ Loading metadata from: data/training/superfib_r1_metadata.pickle\n",
      "ðŸ“Š Available metadata keys: ['max_tgt_length', 'max_src_points', 'max_point_dim', 'src_vocab_list', 'tgt_vocab_list', 'point_num_dist']\n",
      "ðŸ“š Vocabulary sizes from metadata:\n",
      "  - Source vocab: 1002\n",
      "  - Target vocab: 15\n",
      "max_point_dim: 4\n",
      "max_src_points: 80\n",
      "max_tgt_length: 859\n"
     ]
    }
   ],
   "source": [
    "# Load metadata from file (supports YAML, JSON, or pickle)\n",
    "print(f\"\\nðŸ“‹ Loading metadata from: {config[\"metadata_path\"]}\")\n",
    "metadata_path = Path(\"..\")/ config['metadata_path']\n",
    "\n",
    "if not Path(metadata_path).exists():\n",
    "    raise FileNotFoundError(f\"Metadata file not found: {metadata_path}\")\n",
    "\n",
    "# Determine file format from extension\n",
    "file_ext = Path(metadata_path).suffix.lower()\n",
    "\n",
    "\n",
    "if file_ext in ['.yaml', '.yml']:\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = yaml.safe_load(f)\n",
    "    \n",
    "elif file_ext == '.json':\n",
    "    with open(metadata_path, 'r') as f:\n",
    "        metadata = json.load(f)\n",
    "    \n",
    "elif file_ext in ['.pkl', '.pickle']:\n",
    "    with open(metadata_path, 'rb') as f:\n",
    "        metadata = pickle.load(f)\n",
    "    \n",
    "else:\n",
    "    raise ValueError(f\"Unsupported metadata file format: {file_ext}. Supported formats: .yaml, .yml, .json, .pkl, .pickle\")\n",
    "\n",
    "print(f\"ðŸ“Š Available metadata keys: {list(metadata.keys())}\")\n",
    "\n",
    "\n",
    "print(f\"ðŸ“š Vocabulary sizes from metadata:\")\n",
    "print(f\"  - Source vocab: {len(metadata['src_vocab_list'])}\")\n",
    "print(f\"  - Target vocab: {len(metadata['tgt_vocab_list'])}\")\n",
    "print(f\"max_point_dim: {metadata['max_point_dim']}\")\n",
    "print(f\"max_src_points: {metadata['max_src_points']}\")\n",
    "print(f\"max_tgt_length: {metadata['max_tgt_length']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e34ece45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing shard 1/100 with size 71648\n",
      "Train dataset: 35824 samples\n",
      "Validation dataset: 35824 samples\n",
      "Creating length-based groups...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Creating orig to subset idx mapping: 35824it [00:00, 10848862.56it/s]\n",
      "Mapping point_num_dist to subset indices: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 80/80 [00:00<00:00, 32131.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mapped to 80 groups for Subset:\n",
      "  Length 4: 5554 samples (subset indices: [35362, 24771, 31447, 13090, 20282, 7229, 30471, 30077, 14707, 27052]...)\n",
      "  Length 6: 5529 samples (subset indices: [16040, 22130, 21116, 11860, 24499, 15040, 33752, 9624, 32173, 3992]...)\n",
      "  Length 5: 5468 samples (subset indices: [15603, 8047, 29130, 33021, 29597, 35815, 10229, 14297, 8499, 23734]...)\n",
      "  Length 3: 2991 samples (subset indices: [20137, 28665, 3979, 12256, 30255, 7017, 32149, 15509, 31265, 25200]...)\n",
      "  Length 11: 305 samples (subset indices: [7706, 27400, 27265, 12401, 34931, 1628, 19363, 575, 35772, 8595]...)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:626: UserWarning: This DataLoader will create 16 worker processes in total. Our suggested max number of worker in current system is 14 (`cpuset` is not taken into account), which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
      "  warnings.warn(\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n",
      "/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/datasets/table.py:1387: FutureWarning: promote has been superseded by promote_options='default'.\n",
      "  return cls._concat_blocks(pa_tables_to_concat_vertically, axis=0)\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 172, in collate\n    key: collate(\n         ^^^^^^^^\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 207, in collate\n    raise RuntimeError(\"each element in list of batch should be of equal size\")\nRuntimeError: each element in list of batch should be of equal size\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 33\u001b[39m\n\u001b[32m     19\u001b[39m dataloader = CSVDataModule(\n\u001b[32m     20\u001b[39m     data_path = \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m     21\u001b[39m     dataset=mini_dataset,\n\u001b[32m   (...)\u001b[39m\u001b[32m     29\u001b[39m     point_num_dist=point_num_dist,\n\u001b[32m     30\u001b[39m )\n\u001b[32m     32\u001b[39m dataloader.setup()\n\u001b[32m---> \u001b[39m\u001b[32m33\u001b[39m batch = \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     34\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mMiniData \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Source shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch[\u001b[33m'\u001b[39m\u001b[33msource\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, Target shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbatch[\u001b[33m'\u001b[39m\u001b[33mtarget\u001b[39m\u001b[33m'\u001b[39m].shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:733\u001b[39m, in \u001b[36m_BaseDataLoaderIter.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    730\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    731\u001b[39m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[32m    732\u001b[39m     \u001b[38;5;28mself\u001b[39m._reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m733\u001b[39m data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    734\u001b[39m \u001b[38;5;28mself\u001b[39m._num_yielded += \u001b[32m1\u001b[39m\n\u001b[32m    735\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m    736\u001b[39m     \u001b[38;5;28mself\u001b[39m._dataset_kind == _DatasetKind.Iterable\n\u001b[32m    737\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    738\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m._num_yielded > \u001b[38;5;28mself\u001b[39m._IterableDataset_len_called\n\u001b[32m    739\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1515\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._next_data\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1513\u001b[39m worker_id = \u001b[38;5;28mself\u001b[39m._task_info.pop(idx)[\u001b[32m0\u001b[39m]\n\u001b[32m   1514\u001b[39m \u001b[38;5;28mself\u001b[39m._rcvd_idx += \u001b[32m1\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1515\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mworker_id\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/dataloader.py:1550\u001b[39m, in \u001b[36m_MultiProcessingDataLoaderIter._process_data\u001b[39m\u001b[34m(self, data, worker_idx)\u001b[39m\n\u001b[32m   1548\u001b[39m \u001b[38;5;28mself\u001b[39m._try_put_index()\n\u001b[32m   1549\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[32m-> \u001b[39m\u001b[32m1550\u001b[39m     \u001b[43mdata\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1551\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/work/PrfSR/.venv/lib/python3.12/site-packages/torch/_utils.py:750\u001b[39m, in \u001b[36mExceptionWrapper.reraise\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    746\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[32m    747\u001b[39m     \u001b[38;5;66;03m# If the exception takes multiple arguments or otherwise can't\u001b[39;00m\n\u001b[32m    748\u001b[39m     \u001b[38;5;66;03m# be constructed, don't try to instantiate since we don't know how to\u001b[39;00m\n\u001b[32m    749\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m750\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[31mRuntimeError\u001b[39m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/worker.py\", line 349, in _worker_loop\n    data = fetcher.fetch(index)  # type: ignore[possibly-undefined]\n           ^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/fetch.py\", line 55, in fetch\n    return self.collate_fn(data)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 398, in default_collate\n    return collate(batch, collate_fn_map=default_collate_fn_map)\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 172, in collate\n    key: collate(\n         ^^^^^^^^\n  File \"/Users/takeruito/work/PrfSR/.venv/lib/python3.12/site-packages/torch/utils/data/_utils/collate.py\", line 207, in collate\n    raise RuntimeError(\"each element in list of batch should be of equal size\")\nRuntimeError: each element in list of batch should be of equal size\n"
     ]
    }
   ],
   "source": [
    "def source_len(sample):\n",
    "    return {\"source_len\": len(sample[\"source\"])}\n",
    "\n",
    "shuffled_dataset = eval_dataset.shuffle(seed=42)\n",
    "\n",
    "\n",
    "point_num_dist = metadata.get(\"point_num_dist\")\n",
    "\n",
    "mini_data_num = 100\n",
    "\n",
    "for i in range(mini_data_num):\n",
    "    mini_dataset = shuffled_dataset.shard(num_shards=mini_data_num, index=i)\n",
    "    #mini_dataset = mini_dataset.map(source_len, num_proc=1)\n",
    "    #mini_dataset = mini_dataset.sort(\"source_len\")\n",
    "    point_num_dist = defaultdict(list)\n",
    "    for idx, sample in enumerate(mini_dataset):\n",
    "        point_num_dist[len(sample[\"source\"])].append(idx)\n",
    "    print(f\"Processing shard {i+1}/{mini_data_num} with size {len(mini_dataset)}\")\n",
    "    dataloader = CSVDataModule(\n",
    "        data_path = None,\n",
    "        dataset=mini_dataset,\n",
    "        batch_size=config['batch_size'],\n",
    "        num_workers=config['num_workers'],\n",
    "        train_val_split=1 - config[\"test_ratio\"],\n",
    "        seed=42,\n",
    "        batching_strategy=\"length_aware_token\",\n",
    "        min_tokens_per_batch=config['min_n_tokens_in_batch'],\n",
    "        max_batch_size=config['batch_size'],\n",
    "        point_num_dist=point_num_dist,\n",
    "    )\n",
    "    \n",
    "    dataloader.setup()\n",
    "    batch = next(iter(dataloader.train_dataloader()))\n",
    "    print(f\"MiniData {i+1} - Source shape: {batch['source'].shape}, Target shape: {batch['target'].shape}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PrfSR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
